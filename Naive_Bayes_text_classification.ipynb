{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naive Bayes text classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMiZVHBxO1U3ug+0u45f7dN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MykhailoFokin/MachineLearning/blob/master/Naive_Bayes_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgyucsrLYHC4",
        "colab_type": "code",
        "outputId": "5ae75e52-dd40-4412-ee48-bd2d5959f6b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Basic test of algorithm (clear nltk with defaults)\n",
        "# Move further to the next block for full implementation\n",
        "from collections import Counter\n",
        "import itertools\n",
        "import math\n",
        "import nltk\n",
        "import copy\n",
        "# nltk.download(\"popular\")\n",
        "\n",
        "# Train data\n",
        "data = [[\"Chinese Beijing Chinese\",\"0\"],\n",
        "        [\"Chinese Chinese Shanghai\",\"0\"], \n",
        "        [\"Chinese Macao\",\"0\"],\n",
        "        [\"Tokyo Japan Chinese\",\"1\"]]\n",
        "\n",
        "class_count = {}\n",
        "class_count_all = float(len(data))\n",
        "word_count_all = {}\n",
        "class_words_count = {}\n",
        "wc_by_class = {item[1]: {} for item in data}\n",
        "class_probability_template = copy.deepcopy(wc_by_class)\n",
        "class_words_count = {item[1]: float(0) for item in data}\n",
        "for line in data:\n",
        "  if line[1] in class_count:\n",
        "    class_count[line[1]] += 1\n",
        "  else :\n",
        "    class_count[line[1]] = float(1)\n",
        "  tokenized_word=nltk.word_tokenize(line[0])\n",
        "  fdist = nltk.FreqDist(tokenized_word)\n",
        "  #fdist = nltk.FreqDist(w.lower() for w in tokenized_word)\n",
        "  for word in fdist:\n",
        "    #words_all += 1   # increase words counter\n",
        "    class_words_count[line[1]] += fdist[word]\n",
        "    # add/update word to class\n",
        "    if word in wc_by_class[line[1]]:\n",
        "      wc_by_class[line[1]][word] += fdist[word]\n",
        "    else:\n",
        "      wc_by_class[line[1]][word] = float(fdist[word])\n",
        "    # add/update word to summary dict\n",
        "    if word in word_count_all :\n",
        "      word_count_all[word] += fdist[word]\n",
        "    else :\n",
        "      word_count_all[word] = float(fdist[word])\n",
        "  #tokenized_words.append((dict(fdist), {\"class\" : line[1]}))\n",
        "\n",
        "def calculate_conditional_probability(wc,count_c,v):\n",
        "  '''\n",
        "  == Input ==\n",
        "\n",
        "  wc      : certain word count in appropriate class\\n\n",
        "  count_c : overall word count in appropriate class\\n\n",
        "  v       : unique word count in all classes\n",
        "  \n",
        "  == Output ==  \n",
        "  result : probability for certain word for certain class\n",
        "  '''\n",
        "  return (1 + wc)/(count_c + v)\n",
        "\n",
        "# Test\n",
        "new = [\"Chinese Chinese Chinese Tokyo Japan\"]\n",
        "\n",
        "tokenized_word=nltk.word_tokenize(new[0])\n",
        "fdist = nltk.FreqDist(tokenized_word)\n",
        "#print(Counter(tok['KeyName'] for tok in fdist))\n",
        "x = 0\n",
        "for key in wc_by_class:\n",
        "  #print(\"Key of main dictionary: \" + key)\n",
        "  class_probability = class_count[key]/class_count_all\n",
        "  for word in fdist:\n",
        "    p1 = wc_by_class[key][word] if word in wc_by_class[key] else float(0)\n",
        "    p2 = class_words_count[key]\n",
        "    p3 = len(word_count_all)\n",
        "    class_probability *= pow(calculate_conditional_probability(p1,p2,p3),fdist[word])\n",
        "  #print(\"Class\", key, class_probability)\n",
        "  class_probability_template[key] = class_probability\n",
        "\n",
        "print(class_probability_template)\n",
        "print(\"Log10 : {d}\", {k: math.log10(v) for k, v in class_probability_template.items()})\n",
        "print(\"Log : {d}\", {k: math.log(v) for k, v in class_probability_template.items()})\n",
        "print(\"Log2 : {d}\", {k: math.log2(v) for k, v in class_probability_template.items()})\n",
        "print(\"Log1p : {d}\", {k: math.log1p(v) for k, v in class_probability_template.items()})\n",
        "print(\"Exp : {d}\", {k: math.exp(v) for k, v in class_probability_template.items()})\n",
        "print(\"Expm1 : {d}\", {k: math.expm1(v) for k, v in class_probability_template.items()})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'0': 0.00030121377997263036, '1': 0.00013548070246744223}\n",
            "Log10 : {d} {'0': -3.5211251638485592, '1': -3.868122560204681}\n",
            "Log : {d} {'0': -8.107690312843909, '1': -8.906681345001262}\n",
            "Log2 : {d} {'0': -11.696924607403396, '1': -12.849625007211563}\n",
            "Log1p : {d} {'0': 0.00030116842420963296, '1': 0.00013547152578590452}\n",
            "Exp : {d} {'0': 1.0003012591493985, '1': 1.0001354898803922}\n",
            "Expm1 : {d} {'0': 0.0003012591493984375, '1': 0.0001354898803922853}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVQLevCbeY0N",
        "colab_type": "code",
        "outputId": "9a674982-08ce-431e-cc48-c41d40ed9b5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        }
      },
      "source": [
        "# NLTK preparation\n",
        "# Run it to get basic library content for futher usage\n",
        "import nltk\n",
        "nltk.download(\"popular\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXUZHhLhq8e4",
        "colab_type": "code",
        "outputId": "e11a0517-d551-4c8c-fdc7-d7f0c6b1c7ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# install additional dependecy\n",
        "pip install stop-words"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stop-words\n",
            "  Downloading https://files.pythonhosted.org/packages/1c/cb/d58290804b7a4c5daa42abbbe2a93c477ae53e45541b1825e86f0dfaaf63/stop-words-2018.7.23.tar.gz\n",
            "Building wheels for collected packages: stop-words\n",
            "  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stop-words: filename=stop_words-2018.7.23-cp36-none-any.whl size=32916 sha256=164083531a3857e8cf99c345fcec2107fa63769b684520fcdc1ea7ce87c50668\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/37/6a/2b295e03bd07290f0da95c3adb9a74ba95fbc333aa8b0c7c78\n",
            "Successfully built stop-words\n",
            "Installing collected packages: stop-words\n",
            "Successfully installed stop-words-2018.7.23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fekOwd4L6aAT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import block\n",
        "import nltk\n",
        "#from collections import Counter\n",
        "import itertools\n",
        "import math\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "from stop_words import get_stop_words\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "import operator\n",
        "\n",
        "# Define block\n",
        "class NaiveBayes :\n",
        "\n",
        "  def __init__ (self) :\n",
        "    self.ClassCount = {}\n",
        "    self.word_count_all = {}\n",
        "    self.ElementsByClass = {}  #   class_words_count = {}\n",
        "    #wc_by_class = {item[1]: {} for item in data}\n",
        "    # from stop_words import get_stop_words \n",
        "    # stop_words = list(get_stop_words('en'))  # About 900 words\n",
        "    # from nltk.corpus import stopwords\n",
        "    # list(stopwords.words('english')) # About 150 stopwords\n",
        "    # stop_words.extend(nltk_words)\n",
        "    self.stopwords = set(nltk.corpus.stopwords.words(\"english\")) # import stopwords and remove duplicates\n",
        "    self.AccuracyMatrix = []\n",
        "    self.Accuracy = float(0)\n",
        "    self.AccuracyPredictions = []\n",
        "    self.PredictionsProbabilities = []\n",
        "    self.AccuracyPredictionsTest = False\n",
        "\n",
        "    self.change_stopwords_list(\"Extended\")\n",
        "\n",
        "  def change_stopwords_list(self, whatlist = 'Default') :\n",
        "    '''\n",
        "    Optionally we can change default Enlgish stop words from NLTK\n",
        "\n",
        "    == Input ==\n",
        "\n",
        "    whatlist : string parameter for certain stop words list\\n\n",
        "       Default : Use only NLTK list. About 150 stopwords\n",
        "       Alternative : Use stop_words library. About 900 words\n",
        "       Extended : Use both together\n",
        "    '''\n",
        "    if whatlist == 'Default' :\n",
        "      self.stopwords = set(nltk.corpus.stopwords.words(\"english\")) # import stopwords and remove duplicates\n",
        "    elif whatlist == 'Alternative' :\n",
        "      self.stopwords = set(get_stop_words('en')) # import stopwords and remove duplicates\n",
        "    elif whatlist == 'Extended' :\n",
        "      nltkstopwords = list(nltk.corpus.stopwords.words(\"english\"))\n",
        "      stop_words = list(get_stop_words('en'))\n",
        "      stop_words.extend(nltkstopwords)\n",
        "      self.stopwords = set(stop_words)\n",
        "\n",
        "  def tokenize_string(self, input_string, tokenizer_type=0):\n",
        "    '''\n",
        "    Split string into tokens.\n",
        "\n",
        "    == Input ==\n",
        "\n",
        "    input_string : string parameter for certain stop words list\\n\n",
        "    tokenizer_type : how split string into words, what tokenizer rules should it use\\n\n",
        "       0 : Default value, it will use default delimiters with function nltk.word_tokenize\\n\n",
        "       1 : It will picks out sequences of alphanumeric characters as tokens and drops everything else.\\n\n",
        "           nltk.tokenize.RegexpTokenizer function\n",
        "    '''\n",
        "    tokenized_string = []\n",
        "    #print(input_string)\n",
        "    if tokenizer_type == 1:\n",
        "      tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
        "      tokenized_string = tokenizer.tokenize(input_string)\n",
        "    else :\n",
        "      tokenized_string = nltk.word_tokenize(input_string)\n",
        "    #print(tokenized_string)\n",
        "    return tokenized_string\n",
        "\n",
        "  def split_traint_test(self, input_data) :\n",
        "    if (len(input_data)>10000) :\n",
        "      test_size = 2000\n",
        "    else :\n",
        "      test_size = int(len(input_data)*0.2)\n",
        "    #print(len(input_data[:test_size]))\n",
        "    #print(len(input_data[test_size:]))\n",
        "    return input_data[:test_size], input_data[test_size:]\n",
        "\n",
        "  def fit(self, input_data, use_stop_words, use_stemming, use_alphanumeric, print_word_distribution) :\n",
        "    '''\n",
        "    Takes input_data and apply on it Naive Bayes algorithm.\\n\n",
        "    Input will be separated depends on how many elements in the list,\\n\n",
        "    not more than 2,000 elements will be taken as a test list.\\n\n",
        "    The result of this method is accuracy of predictions based on splitting\\n\n",
        "    Input dataset into train and test and applying algorithm on it.\n",
        "\n",
        "    == Input ==\n",
        "\n",
        "    input_data : string parameter for certain stop words list\\n\n",
        "    use_stop_words   : (False,True) Should be apply filtering out stopwords\n",
        "    use_stemming     : (False,True) Should it also uses lexicon(linguistic) normalization\n",
        "    use_alphanumeric : (False,True) Should non alphanumeric symbols be excluded\n",
        "    '''\n",
        "    #self.ClassCount = \n",
        "    self.ClassCountAll = float(len(input_data))  # class_count_all = float(len(data))\n",
        "    self.ElementsByClass = {item[1]: float(0) for item in input_data}\n",
        "    self.WordsByClass = {item[1]: {} for item in input_data}\n",
        "    self.ClassProbabilityResult = copy.deepcopy(self.WordsByClass)\n",
        "    self.UseStopWords = use_stop_words\n",
        "    self.UseStemming = use_stemming\n",
        "    self.AlphaNumeric = 1 if use_alphanumeric else 0\n",
        "    self.AccuracyMatrix = []\n",
        "    self.AccuracyPredictions = []\n",
        "    self.AccuracyPredictionsTest = False\n",
        "    self.word_count_all = {}\n",
        "\n",
        "    self.test_data, self.train_data = self.split_traint_test(input_data)\n",
        "\n",
        "    #print(self.train_data)\n",
        "    #for line in input_data:\n",
        "    for line in self.train_data:\n",
        "      #print(line[0])\n",
        "      if line[1] in self.ClassCount:\n",
        "        self.ClassCount[line[1]] += 1\n",
        "      else :\n",
        "        self.ClassCount[line[1]] = float(1)\n",
        "      #tokenized_word = nltk.word_tokenize(line[0])\n",
        "      tokenized_word = self.tokenize_string(line[0],1)\n",
        "      #print(line[0])\n",
        "      #print(tokenized_word)\n",
        "      \n",
        "      #filtered_words = [word for word in tokenized_word if word not in stopwords.words('english')]\n",
        "      if use_stop_words :\n",
        "        tokenized_word = self.removing_stop_words(tokenized_word)\n",
        "      if use_stemming :\n",
        "        tokenized_word = self.stemming(tokenized_word)\n",
        "      fdist = nltk.FreqDist(tokenized_word)\n",
        "      #print(dict(fdist))\n",
        "      \n",
        "      #fdist = nltk.FreqDist(w.lower() for w in filtered_words) # stopwords in nltk in lowercase\n",
        "      #filtered_words = [word for word in fdist if word not in stopwords.words('english')]\n",
        "      for word in fdist:\n",
        "        self.ElementsByClass[line[1]] += fdist[word]\n",
        "        # add/update word to class\n",
        "        if word in self.WordsByClass[line[1]]:\n",
        "          self.WordsByClass[line[1]][word] += fdist[word]\n",
        "        else:\n",
        "          self.WordsByClass[line[1]][word] = float(fdist[word])\n",
        "        # add/update word to summary dict\n",
        "        if word in self.word_count_all :\n",
        "          self.word_count_all[word] += fdist[word]\n",
        "        else :\n",
        "          self.word_count_all[word] = float(fdist[word])\n",
        "    \n",
        "    # Prediction block (Predict, Evaluate, Calculate accuracy)\n",
        "    #accuracy_matrix = []\n",
        "    #for test_line in self.test_data:\n",
        "    #  prediction_probabilities = self.predict_one(test_line)\n",
        "    #  self.AccuracyPredictions.append(max(prediction_probabilities.items(), key=operator.itemgetter(1))[0])\n",
        "    #  self.AccuracyMatrix.append(test_line[1] == max(prediction_probabilities.items(), key=operator.itemgetter(1))[0])\n",
        "    #self.Accuracy = np.sum(self.AccuracyMatrix)/len(self.AccuracyMatrix)\n",
        "\n",
        "    #print(dict(fdist))\n",
        "    if print_word_distribution:\n",
        "      self.print_freq_distrinbution(self.word_count_all)\n",
        "    self.predict(self.test_data)\n",
        "    self.AccuracyPredictionsTest = copy.deepcopy(self.AccuracyPredictions)\n",
        "    self.AccuracyMatrixTest = copy.deepcopy(self.AccuracyMatrix)\n",
        "\n",
        "    return self.Accuracy\n",
        "\n",
        "  def print_errors(self, accuracy_matrix, predictions_matrix, source_data):\n",
        "    if accuracy_matrix : # check that it is not empty\n",
        "      print(\"Errors during training : \")\n",
        "      for i in range(len(accuracy_matrix)) :\n",
        "        print(\"Source class : {d} , Predicted class : {d}, Message : {s}\", source_data[i][1], predictions_matrix[i], source_data[i][0])\n",
        "\n",
        "  def print_training_errors(self):\n",
        "    self.print_errors(self, self.AccuracyMatrixTest, self.AccuracyPredictionsTest, self.test_data)\n",
        "\n",
        "  def predict(self, input_data) :\n",
        "    self.PredictionsProbabilities = []\n",
        "    self.AccuracyPredictions = []\n",
        "    self.AccuracyMatrix = []\n",
        "    for test_line in input_data:\n",
        "      prediction_probabilities = self.predict_one(test_line)\n",
        "      self.PredictionsProbabilities.append(prediction_probabilities)\n",
        "      self.AccuracyPredictions.append(max(prediction_probabilities.items(), key=operator.itemgetter(1))[0])\n",
        "      self.AccuracyMatrix.append(test_line[1] == max(prediction_probabilities.items(), key=operator.itemgetter(1))[0])\n",
        "    self.Accuracy = np.sum(self.AccuracyMatrix)/len(self.AccuracyMatrix)\n",
        "    if self.AccuracyPredictionsTest :\n",
        "      #print_errors(self.AccuracyMatrix, self.AccuracyPredictionsTest, input_data) # if this isn't called from FIT then print predictions\n",
        "      print(self.PredictionsProbabilities)\n",
        "\n",
        "  def predict_one(self, input_string) :\n",
        "    tokenized_test = self.tokenize_string(input_string[0],1)\n",
        "    if self.UseStopWords :\n",
        "      tokenized_test = self.removing_stop_words(tokenized_test)\n",
        "    if self.UseStemming :\n",
        "      tokenized_test = self.stemming(tokenized_test)\n",
        "    fdist_test = nltk.FreqDist(tokenized_test)\n",
        "    #tokenized_word=nltk.word_tokenize(new[0])\n",
        "    #fdist = nltk.FreqDist(tokenized_word)\n",
        "    #x = 0\n",
        "    for key in self.WordsByClass:\n",
        "      self.ClassProbabilityResult[key] = float(0) # set to 0 if there is previuos result \n",
        "      class_probability = self.ClassCount[key]/self.ClassCountAll\n",
        "      for word in fdist_test:\n",
        "        p1 = self.WordsByClass[key][word] if word in self.WordsByClass[key] else float(0)\n",
        "        p2 = self.ElementsByClass[key]\n",
        "        p3 = len(self.word_count_all)\n",
        "        class_probability *= pow(self.calculate_conditional_probability(p1,p2,p3),fdist_test[word])\n",
        "      self.ClassProbabilityResult[key] = math.log10(class_probability)\n",
        "    return self.ClassProbabilityResult\n",
        "\n",
        "  def print_freq_distrinbution(self, fdist):\n",
        "    # Frequency Distribution Plot\n",
        "    #FdistPrint = fdist.most_common(20) # take 20 most common words\n",
        "    #fdist.plot(30,cumulative=False)\n",
        "    #print(fdist)\n",
        "    lists = sorted(fdist.items())\n",
        "    x, y = zip(*lists)\n",
        "    plt.plot(x, y)\n",
        "    plt.show()\n",
        "\n",
        "  def removing_stop_words(self, tokenized_string) :\n",
        "    '''\n",
        "    Removing stop words from tokenized string (dict of words)\n",
        "\n",
        "    == Input ==\n",
        "\n",
        "    tokenized_string : dict of tokens (dictionary of words from splitted string)\n",
        "    \n",
        "    == Output ==  \n",
        "\n",
        "    filtered_tokens : dict of filtered words in lowercase\n",
        "    '''\n",
        "    filtered_tokens = []\n",
        "    for word in tokenized_string:\n",
        "      word = word.lower() # in case they arenet all lower cased\n",
        "      if word not in self.stopwords : #.words(\"english\"):\n",
        "        filtered_tokens.append(word)\n",
        "    #return [word for word in word_list if word not in self.stopwords.words('english')]\n",
        "    return filtered_tokens\n",
        "  \n",
        "  def stemming(self, tokenized_string) :\n",
        "    '''\n",
        "    Stemming\\n\n",
        "    Stemming is a process of linguistic normalization, which reduces words\n",
        "    to their word root word or chops off the derivational affixes.\n",
        "    For example, connection, connected, connecting word reduce to a common word \"connect\".\n",
        "\n",
        "    == Input ==\n",
        "\n",
        "    fdsit : dict of tokens (dictionary of words from splitted string)\n",
        "    \n",
        "    == Output ==  \n",
        "\n",
        "    stemmed_tokens : dict of preprocessed words\n",
        "    '''\n",
        "\n",
        "    ps = nltk.stem.PorterStemmer()\n",
        "\n",
        "    stemmed_words=[]\n",
        "    for w in tokenized_string:\n",
        "        stemmed_words.append(ps.stem(w))\n",
        "\n",
        "    return stemmed_words\n",
        "\n",
        "  def calculate_conditional_probability(self,wc,count_c,v):\n",
        "    '''\n",
        "    == Input ==\n",
        "\n",
        "    wc      : certain word count in appropriate class\\n\n",
        "    count_c : overall word count in appropriate class\\n\n",
        "    v       : unique word count in all classes\n",
        "    \n",
        "    == Output ==  \n",
        "    result : probability for certain word for certain class\n",
        "    '''\n",
        "    return (1 + wc)/(count_c + v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfX5VvAQj59_",
        "colab_type": "code",
        "outputId": "4ccf12bc-8f78-4a35-f209-c78ef7904d57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Test of sample data\n",
        "# Part 1\n",
        "# Copy test data from repo (do it once)\n",
        "!git clone https://github.com/MykhailoFokin/MachineLearning.git"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'MachineLearning'...\n",
            "remote: Enumerating objects: 27, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 148 (delta 14), reused 0 (delta 0), pack-reused 121\u001b[K\n",
            "Receiving objects: 100% (148/148), 39.65 MiB | 9.47 MiB/s, done.\n",
            "Resolving deltas: 100% (80/80), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXd6CrDqlBoi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test of sample data\n",
        "# Part 2\n",
        "source_data_list = []\n",
        "\n",
        "with open('MachineLearning/Naive_Bayes_data_test.csv', 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    for row in reader :\n",
        "      source_data_list.append(row)\n",
        "    #source_data_list = list(reader) # Train data\n",
        "\n",
        "# Create instance of NaiveBayes class\n",
        "nb = NaiveBayes()\n",
        "\n",
        "# Train our model\n",
        "# Tips: inside fit method it would be nice to split input data into train / test (80/20) sets and return modelâ€™ accuracy, e.g.:\n",
        "Accuracy = nb.fit(source_data_list, False, False, False, True)  # return accuracy\n",
        "print(\"Accuracy : {d} \", Accuracy) \n",
        "#Accuracy = nb.fit(source_data_list, True, False, False, True)  # return accuracy \n",
        "#print(\"Accuracy with stopwords : {d} \", Accuracy)\n",
        "#Accuracy = nb.fit(source_data_list, True, True, False, True)  # return accuracy \n",
        "#print(\"Accuracy with stopwords, stemming: {d} \", Accuracy)\n",
        "#Accuracy = nb.fit(source_data_list, True, True, True, True)  # return accuracy \n",
        "#print(\"Accuracy with stopwords, stemming, filtering out non alphanumeric symbols: {d} \", Accuracy)\n",
        "\n",
        "# Try to predict class of text\n",
        "nb.predict([\"Chinese Chinese Chinese Tokyo Japan\"])\n",
        "\n",
        "# Must return[ ('Chinese Chinese Chinese Tokyo Japan', '0')]\n",
        "# pobability {'1': 0.00013548070246744226, '0': 0.00030121377997263036}\n",
        "# or log     {'1': -7.906681345001262, '0': -7.10769031284391}\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}