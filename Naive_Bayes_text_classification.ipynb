{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naive Bayes text classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMOAJ/VTdjjbOw/4J5i8FRQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MykhailoFokin/MachineLearning/blob/master/Naive_Bayes_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgyucsrLYHC4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "f9cec1a4-1950-413a-a96d-afa46b650de9"
      },
      "source": [
        "# Basic test of algorithm (clear nltk with defaults)\n",
        "# Move further to the next block for full implementation\n",
        "from collections import Counter\n",
        "import itertools\n",
        "import math\n",
        "import nltk\n",
        "import copy\n",
        "# nltk.download(\"popular\")\n",
        "\n",
        "# Train data\n",
        "data = [[\"Chinese Beijing Chinese\",\"0\"],\n",
        "        [\"Chinese Chinese Shanghai\",\"0\"], \n",
        "        [\"Chinese Macao\",\"0\"],\n",
        "        [\"Tokyo Japan Chinese\",\"1\"]]\n",
        "\n",
        "class_count = {}\n",
        "class_count_all = float(len(data))\n",
        "word_count_all = {}\n",
        "class_words_count = {}\n",
        "wc_by_class = {item[1]: {} for item in data}\n",
        "class_probability_template = copy.deepcopy(wc_by_class)\n",
        "class_words_count = {item[1]: float(0) for item in data}\n",
        "for line in data:\n",
        "  if line[1] in class_count:\n",
        "    class_count[line[1]] += 1\n",
        "  else :\n",
        "    class_count[line[1]] = float(1)\n",
        "  tokenized_word=nltk.word_tokenize(line[0])\n",
        "  fdist = nltk.FreqDist(tokenized_word)\n",
        "  #fdist = nltk.FreqDist(w.lower() for w in tokenized_word)\n",
        "  for word in fdist:\n",
        "    #words_all += 1   # increase words counter\n",
        "    class_words_count[line[1]] += fdist[word]\n",
        "    # add/update word to class\n",
        "    if word in wc_by_class[line[1]]:\n",
        "      wc_by_class[line[1]][word] += fdist[word]\n",
        "    else:\n",
        "      wc_by_class[line[1]][word] = float(fdist[word])\n",
        "    # add/update word to summary dict\n",
        "    if word in word_count_all :\n",
        "      word_count_all[word] += fdist[word]\n",
        "    else :\n",
        "      word_count_all[word] = float(fdist[word])\n",
        "  #tokenized_words.append((dict(fdist), {\"class\" : line[1]}))\n",
        "\n",
        "def calculate_conditional_probability(wc,count_c,v):\n",
        "  '''\n",
        "  == Input ==\n",
        "\n",
        "  wc      : certain word count in appropriate class\\n\n",
        "  count_c : overall word count in appropriate class\\n\n",
        "  v       : unique word count in all classes\n",
        "  \n",
        "  == Output ==  \n",
        "  result : probability for certain word for certain class\n",
        "  '''\n",
        "  return (1 + wc)/(count_c + v)\n",
        "\n",
        "# Test\n",
        "new = [\"Chinese Chinese Chinese Tokyo Japan\"]\n",
        "\n",
        "tokenized_word=nltk.word_tokenize(new[0])\n",
        "fdist = nltk.FreqDist(tokenized_word)\n",
        "#print(Counter(tok['KeyName'] for tok in fdist))\n",
        "x = 0\n",
        "for key in wc_by_class:\n",
        "  #print(\"Key of main dictionary: \" + key)\n",
        "  class_probability = class_count[key]/class_count_all\n",
        "  for word in fdist:\n",
        "    p1 = wc_by_class[key][word] if word in wc_by_class[key] else float(0)\n",
        "    p2 = class_words_count[key]\n",
        "    p3 = len(word_count_all)\n",
        "    class_probability *= pow(calculate_conditional_probability(p1,p2,p3),fdist[word])\n",
        "  #print(\"Class\", key, class_probability)\n",
        "  class_probability_template[key] = class_probability\n",
        "\n",
        "print(class_probability_template)\n",
        "print(\"Log10 : {d}\", {k: math.log10(v) for k, v in class_probability_template.items()})\n",
        "print(\"Log : {d}\", {k: math.log(v) for k, v in class_probability_template.items()})\n",
        "print(\"Log2 : {d}\", {k: math.log2(v) for k, v in class_probability_template.items()})\n",
        "print(\"Log1p : {d}\", {k: math.log1p(v) for k, v in class_probability_template.items()})\n",
        "print(\"Exp : {d}\", {k: math.exp(v) for k, v in class_probability_template.items()})\n",
        "print(\"Expm1 : {d}\", {k: math.expm1(v) for k, v in class_probability_template.items()})"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-0cc9cab95cca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfdist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwc_by_class\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwc_by_class\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mp2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_words_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mp3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_count_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/collections/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'expected at most 1 arguments, got %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__missing__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/collections/__init__.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    620\u001b[0m                     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fast path when counter is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m                 \u001b[0m_count_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-70-0cc9cab95cca>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfdist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwc_by_class\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwc_by_class\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mp2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_words_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mp3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_count_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVQLevCbeY0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NLTK preparation\n",
        "# Run it to get basic library content for futher usage\n",
        "import nltk\n",
        "nltk.download(\"popular\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fekOwd4L6aAT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import block\n",
        "import nltk\n",
        "#from collections import Counter\n",
        "import itertools\n",
        "import math\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "from stop_words import get_stop_words\n",
        "\n",
        "# Define block\n",
        "class NaiveBayes :\n",
        "\n",
        "  def __init__ (self) :\n",
        "    self.ClassCount = {}\n",
        "    word_count_all = {}\n",
        "    self.ElementsByClass = {}  #   class_words_count = {}\n",
        "    #wc_by_class = {item[1]: {} for item in data}\n",
        "    # from stop_words import get_stop_words \n",
        "    # stop_words = list(get_stop_words('en'))  # About 900 words\n",
        "    # from nltk.corpus import stopwords\n",
        "    # list(stopwords.words('english')) # About 150 stopwords\n",
        "    # stop_words.extend(nltk_words)\n",
        "    self.stopwords = set(nltk.corpus.stopwords.words(\"english\")) # import stopwords and remove duplicates\n",
        "\n",
        "  def change_stopwords_list(self, whatlist = 'Default') :\n",
        "    '''\n",
        "    Optionally we can change default Enlgish stop words from NLTK\n",
        "\n",
        "    == Input ==\n",
        "\n",
        "    whatlist : string parameter for certain stop words list\\n\n",
        "       Default : Use only NLTK list. About 150 stopwords\n",
        "       Alternative : Use stop_words library. About 900 words\n",
        "       Extended : Use both together\n",
        "    '''\n",
        "    if whatlist == 'Default' :\n",
        "      self.stopwords = set(nltk.corpus.stopwords.words(\"english\")) # import stopwords and remove duplicates\n",
        "    elif whatlist == 'Alternative' :\n",
        "      self.stopwords = set(get_stop_words('en')) # import stopwords and remove duplicates\n",
        "    elif whatlist == 'Extended' :\n",
        "      nltkstopwords = list(nltk.corpus.stopwords.words(\"english\"))\n",
        "      stop_words = list(get_stop_words('en'))\n",
        "      stop_words.extend(nltk_words)\n",
        "      self.stopwords = set(stop_words)\n",
        "\n",
        "  def fit(self, input_data, use_stop_words, use_stemming, use_lowercase) :\n",
        "    #self.ClassCount = \n",
        "    self.ClassCountAll = float(len(input_data))  # class_count_all = float(len(data))\n",
        "    self.ElementsByClass = {item[1]: float(0) for item in data}\n",
        "    self.WordsByClass = {item[1]: {} for item in data}\n",
        "    self.ClassProbabilityResult = copy.deepcopy(self.WordsByClass)\n",
        "    self.UseStopWords = use_stop_words\n",
        "    self.UseStemming = use_stemming\n",
        "    self.UseLowercase = use_lowercase\n",
        "\n",
        "    for line in input_data:\n",
        "      if line[1] in self.ClassCount:\n",
        "        self.ClassCount[line[1]] += 1\n",
        "      else :\n",
        "        self.ClassCount[line[1]] = float(1)\n",
        "      tokenized_word = nltk.word_tokenize(line[0])\n",
        "      #filtered_words = [word for word in tokenized_word if word not in stopwords.words('english')]\n",
        "      fdist = nltk.FreqDist(w.lower() for w in tokenized_word) # stopwords in nltk in lowercase\n",
        "      #fdist = nltk.FreqDist(w.lower() for w in filtered_words) # stopwords in nltk in lowercase\n",
        "      filtered_words = [word for word in fdist if word not in stopwords.words('english')]\n",
        "    for word in fdist:\n",
        "      self.ElementsByClass[line[1]] += fdist[word]\n",
        "      # add/update word to class\n",
        "      if word in self.WordsByClass[line[1]]:\n",
        "        self.WordsByClass[line[1]][word] += fdist[word]\n",
        "      else:\n",
        "        self.WordsByClass[line[1]][word] = float(fdist[word])\n",
        "      # add/update word to summary dict\n",
        "      if word in word_count_all :\n",
        "        word_count_all[word] += fdist[word]\n",
        "      else :\n",
        "        word_count_all[word] = float(fdist[word])\n",
        "\n",
        "tokenized_word=nltk.word_tokenize(new[0])\n",
        "fdist = nltk.FreqDist(tokenized_word)\n",
        "#print(Counter(tok['KeyName'] for tok in fdist))\n",
        "x = 0\n",
        "for key in self.WordsByClass:\n",
        "  #print(\"Key of main dictionary: \" + key)\n",
        "  class_probability = self.ClassCount[key]/self.ClassCountAll\n",
        "  for word in fdist:\n",
        "    p1 = self.WordsByClass[key][word] if word in self.WordsByClass[key] else float(0)\n",
        "    p2 = self.ElementsByClass[key]\n",
        "    p3 = len(word_count_all)\n",
        "    class_probability *= pow(calculate_conditional_probability(p1,p2,p3),fdist[word])\n",
        "  #print(\"Class\", key, class_probability)\n",
        "  self.ClassProbabilityResult[key] = class_probability\n",
        "\n",
        "  def print_freq_distrinbution(self, fdist):\n",
        "    # Frequency Distribution Plot\n",
        "    #FdistPrint = fdist.most_common(20) # take 20 most common words\n",
        "    fdist.plot(30,cumulative=False)\n",
        "    plt.show()\n",
        "\n",
        "  def removing_stop_words(self, tokenized_string) :\n",
        "     '''\n",
        "    Removing stop words from base\n",
        "\n",
        "    == Input ==\n",
        "\n",
        "    fdsit : dict of tokens (dictionary of words from splitted string)\n",
        "    \n",
        "    == Output ==  \n",
        "\n",
        "    stemmed_tokens : dict of preprocessed words\n",
        "    '''   \n",
        "    processed_word_list = []\n",
        "    for word in tokenized_string:\n",
        "        word = word.lower() # in case they arenet all lower cased\n",
        "        if word not in self.stopwords.words(\"english\"):\n",
        "            processed_word_list.append(word)\n",
        "    return processed_word_list\n",
        "    #return [word for word in word_list if word not in self.stopwords.words('english')]\n",
        "  \n",
        "  def stemming(self, tokenized_string) :\n",
        "    '''\n",
        "    Stemming\\n\n",
        "    Stemming is a process of linguistic normalization, which reduces words\n",
        "    to their word root word or chops off the derivational affixes.\n",
        "    For example, connection, connected, connecting word reduce to a common word \"connect\".\n",
        "\n",
        "    == Input ==\n",
        "\n",
        "    fdsit : dict of tokens (dictionary of words from splitted string)\n",
        "    \n",
        "    == Output ==  \n",
        "\n",
        "    stemmed_tokens : dict of preprocessed words\n",
        "    '''\n",
        "\n",
        "    ps = nltk.stem.PorterStemmer()\n",
        "\n",
        "    stemmed_words=[]\n",
        "    for w in tokenized_string:\n",
        "        stemmed_words.append(ps.stem(w))\n",
        "\n",
        "    return stemmed_words\n",
        "\n",
        "   \n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}