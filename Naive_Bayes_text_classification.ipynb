{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naive Bayes text classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMiZVHBxO1U3ug+0u45f7dN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MykhailoFokin/MachineLearning/blob/master/Naive_Bayes_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgyucsrLYHC4",
        "colab_type": "code",
        "outputId": "5ae75e52-dd40-4412-ee48-bd2d5959f6b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Basic test of algorithm (clear nltk with defaults)\n",
        "# Move further to the next block for full implementation\n",
        "from collections import Counter\n",
        "import itertools\n",
        "import math\n",
        "import nltk\n",
        "import copy\n",
        "# nltk.download(\"popular\")\n",
        "\n",
        "# Train data\n",
        "data = [[\"Chinese Beijing Chinese\",\"0\"],\n",
        "        [\"Chinese Chinese Shanghai\",\"0\"], \n",
        "        [\"Chinese Macao\",\"0\"],\n",
        "        [\"Tokyo Japan Chinese\",\"1\"]]\n",
        "\n",
        "class_count = {}\n",
        "class_count_all = float(len(data))\n",
        "word_count_all = {}\n",
        "class_words_count = {}\n",
        "wc_by_class = {item[1]: {} for item in data}\n",
        "class_probability_template = copy.deepcopy(wc_by_class)\n",
        "class_words_count = {item[1]: float(0) for item in data}\n",
        "for line in data:\n",
        "  if line[1] in class_count:\n",
        "    class_count[line[1]] += 1\n",
        "  else :\n",
        "    class_count[line[1]] = float(1)\n",
        "  tokenized_word=nltk.word_tokenize(line[0])\n",
        "  fdist = nltk.FreqDist(tokenized_word)\n",
        "  #fdist = nltk.FreqDist(w.lower() for w in tokenized_word)\n",
        "  for word in fdist:\n",
        "    #words_all += 1   # increase words counter\n",
        "    class_words_count[line[1]] += fdist[word]\n",
        "    # add/update word to class\n",
        "    if word in wc_by_class[line[1]]:\n",
        "      wc_by_class[line[1]][word] += fdist[word]\n",
        "    else:\n",
        "      wc_by_class[line[1]][word] = float(fdist[word])\n",
        "    # add/update word to summary dict\n",
        "    if word in word_count_all :\n",
        "      word_count_all[word] += fdist[word]\n",
        "    else :\n",
        "      word_count_all[word] = float(fdist[word])\n",
        "  #tokenized_words.append((dict(fdist), {\"class\" : line[1]}))\n",
        "\n",
        "def calculate_conditional_probability(wc,count_c,v):\n",
        "  '''\n",
        "  == Input ==\n",
        "\n",
        "  wc      : certain word count in appropriate class\\n\n",
        "  count_c : overall word count in appropriate class\\n\n",
        "  v       : unique word count in all classes\n",
        "  \n",
        "  == Output ==  \n",
        "  result : probability for certain word for certain class\n",
        "  '''\n",
        "  return (1 + wc)/(count_c + v)\n",
        "\n",
        "# Test\n",
        "new = [\"Chinese Chinese Chinese Tokyo Japan\"]\n",
        "\n",
        "tokenized_word=nltk.word_tokenize(new[0])\n",
        "fdist = nltk.FreqDist(tokenized_word)\n",
        "#print(Counter(tok['KeyName'] for tok in fdist))\n",
        "x = 0\n",
        "for key in wc_by_class:\n",
        "  #print(\"Key of main dictionary: \" + key)\n",
        "  class_probability = class_count[key]/class_count_all\n",
        "  for word in fdist:\n",
        "    p1 = wc_by_class[key][word] if word in wc_by_class[key] else float(0)\n",
        "    p2 = class_words_count[key]\n",
        "    p3 = len(word_count_all)\n",
        "    class_probability *= pow(calculate_conditional_probability(p1,p2,p3),fdist[word])\n",
        "  #print(\"Class\", key, class_probability)\n",
        "  class_probability_template[key] = class_probability\n",
        "\n",
        "print(class_probability_template)\n",
        "print(\"Log10 : {d}\", {k: math.log10(v) for k, v in class_probability_template.items()})\n",
        "print(\"Log : {d}\", {k: math.log(v) for k, v in class_probability_template.items()})\n",
        "print(\"Log2 : {d}\", {k: math.log2(v) for k, v in class_probability_template.items()})\n",
        "print(\"Log1p : {d}\", {k: math.log1p(v) for k, v in class_probability_template.items()})\n",
        "print(\"Exp : {d}\", {k: math.exp(v) for k, v in class_probability_template.items()})\n",
        "print(\"Expm1 : {d}\", {k: math.expm1(v) for k, v in class_probability_template.items()})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'0': 0.00030121377997263036, '1': 0.00013548070246744223}\n",
            "Log10 : {d} {'0': -3.5211251638485592, '1': -3.868122560204681}\n",
            "Log : {d} {'0': -8.107690312843909, '1': -8.906681345001262}\n",
            "Log2 : {d} {'0': -11.696924607403396, '1': -12.849625007211563}\n",
            "Log1p : {d} {'0': 0.00030116842420963296, '1': 0.00013547152578590452}\n",
            "Exp : {d} {'0': 1.0003012591493985, '1': 1.0001354898803922}\n",
            "Expm1 : {d} {'0': 0.0003012591493984375, '1': 0.0001354898803922853}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVQLevCbeY0N",
        "colab_type": "code",
        "outputId": "9a674982-08ce-431e-cc48-c41d40ed9b5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        }
      },
      "source": [
        "# NLTK preparation\n",
        "# Run it to get basic library content for futher usage\n",
        "import nltk\n",
        "nltk.download(\"popular\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXUZHhLhq8e4",
        "colab_type": "code",
        "outputId": "e11a0517-d551-4c8c-fdc7-d7f0c6b1c7ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# install additional dependecy\n",
        "pip install stop-words"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stop-words\n",
            "  Downloading https://files.pythonhosted.org/packages/1c/cb/d58290804b7a4c5daa42abbbe2a93c477ae53e45541b1825e86f0dfaaf63/stop-words-2018.7.23.tar.gz\n",
            "Building wheels for collected packages: stop-words\n",
            "  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stop-words: filename=stop_words-2018.7.23-cp36-none-any.whl size=32916 sha256=164083531a3857e8cf99c345fcec2107fa63769b684520fcdc1ea7ce87c50668\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/37/6a/2b295e03bd07290f0da95c3adb9a74ba95fbc333aa8b0c7c78\n",
            "Successfully built stop-words\n",
            "Installing collected packages: stop-words\n",
            "Successfully installed stop-words-2018.7.23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fekOwd4L6aAT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import block\n",
        "import nltk\n",
        "#from collections import Counter\n",
        "import itertools\n",
        "import math\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "from stop_words import get_stop_words\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "import operator\n",
        "\n",
        "# Define block\n",
        "class NaiveBayes :\n",
        "\n",
        "  def __init__ (self) :\n",
        "    self.ClassCount = {}\n",
        "    self.word_count_all = {}\n",
        "    self.ElementsByClass = {}  #   class_words_count = {}\n",
        "    #wc_by_class = {item[1]: {} for item in data}\n",
        "    # from stop_words import get_stop_words \n",
        "    # stop_words = list(get_stop_words('en'))  # About 900 words\n",
        "    # from nltk.corpus import stopwords\n",
        "    # list(stopwords.words('english')) # About 150 stopwords\n",
        "    # stop_words.extend(nltk_words)\n",
        "    self.stopwords = set(nltk.corpus.stopwords.words(\"english\")) # import stopwords and remove duplicates\n",
        "    self.AccuracyMatrix = []\n",
        "    self.Accuracy = float(0)\n",
        "    self.AccuracyPredictions = []\n",
        "    self.PredictionsProbabilities = []\n",
        "    self.AccuracyPredictionsTest = False\n",
        "\n",
        "    self.change_stopwords_list(\"Extended\")\n",
        "\n",
        "  def change_stopwords_list(self, whatlist = 'Default') :\n",
        "    '''\n",
        "    Optionally we can change default Enlgish stop words from NLTK\n",
        "\n",
        "    == Input ==\n",
        "\n",
        "    whatlist : string parameter for certain stop words list\\n\n",
        "       Default : Use only NLTK list. About 150 stopwords\n",
        "       Alternative : Use stop_words library. About 900 words\n",
        "       Extended : Use both together\n",
        "    '''\n",
        "    if whatlist == 'Default' :\n",
        "      self.stopwords = set(nltk.corpus.stopwords.words(\"english\")) # import stopwords and remove duplicates\n",
        "    elif whatlist == 'Alternative' :\n",
        "      self.stopwords = set(get_stop_words('en')) # import stopwords and remove duplicates\n",
        "    elif whatlist == 'Extended' :\n",
        "      nltkstopwords = list(nltk.corpus.stopwords.words(\"english\"))\n",
        "      stop_words = list(get_stop_words('en'))\n",
        "      stop_words.extend(nltkstopwords)\n",
        "      self.stopwords = set(stop_words)\n",
        "\n",
        "  def tokenize_string(self, input_string, tokenizer_type=0):\n",
        "    '''\n",
        "    Split string into tokens.\n",
        "\n",
        "    == Input ==\n",
        "\n",
        "    input_string : string parameter for certain stop words list\\n\n",
        "    tokenizer_type : how split string into words, what tokenizer rules should it use\\n\n",
        "       0 : Default value, it will use default delimiters with function nltk.word_tokenize\\n\n",
        "       1 : It will picks out sequences of alphanumeric characters as tokens and drops everything else.\\n\n",
        "           nltk.tokenize.RegexpTokenizer function\n",
        "    '''\n",
        "    tokenized_string = []\n",
        "    #print(input_string)\n",
        "    if tokenizer_type == 1:\n",
        "      tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
        "      tokenized_string = tokenizer.tokenize(input_string)\n",
        "    else :\n",
        "      tokenized_string = nltk.word_tokenize(input_string)\n",
        "    #print(tokenized_string)\n",
        "    return tokenized_string\n",
        "\n",
        "  def split_traint_test(self, input_data) :\n",
        "    if (len(input_data)>10000) :\n",
        "      test_size = 2000\n",
        "    else :\n",
        "      test_size = int(len(input_data)*0.2)\n",
        "    #print(len(input_data[:test_size]))\n",
        "    #print(len(input_data[test_size:]))\n",
        "    return input_data[:test_size], input_data[test_size:]\n",
        "\n",
        "  def fit(self, input_data, use_stop_words, use_stemming, use_alphanumeric, print_word_distribution) :\n",
        "    '''\n",
        "    Takes input_data and apply on it Naive Bayes algorithm.\\n\n",
        "    Input will be separated depends on how many elements in the list,\\n\n",
        "    not more than 2,000 elements will be taken as a test list.\\n\n",
        "    The result of this method is accuracy of predictions based on splitting\\n\n",
        "    Input dataset into train and test and applying algorithm on it.\n",
        "\n",
        "    == Input ==\n",
        "\n",
        "    input_data : string parameter for certain stop words list\\n\n",
        "    use_stop_words   : (False,True) Should be apply filtering out stopwords\n",
        "    use_stemming     : (False,True) Should it also uses lexicon(linguistic) normalization\n",
        "    use_alphanumeric : (False,True) Should non alphanumeric symbols be excluded\n",
        "    '''\n",
        "    #self.ClassCount = \n",
        "    self.ClassCountAll = float(len(input_data))  # class_count_all = float(len(data))\n",
        "    self.ElementsByClass = {item[1]: float(0) for item in input_data}\n",
        "    self.WordsByClass = {item[1]: {} for item in input_data}\n",
        "    self.ClassProbabilityResult = copy.deepcopy(self.WordsByClass)\n",
        "    self.UseStopWords = use_stop_words\n",
        "    self.UseStemming = use_stemming\n",
        "    self.AlphaNumeric = 1 if use_alphanumeric else 0\n",
        "    self.AccuracyMatrix = []\n",
        "    self.AccuracyPredictions = []\n",
        "    self.AccuracyPredictionsTest = False\n",
        "    self.word_count_all = {}\n",
        "\n",
        "    self.test_data, self.train_data = self.split_traint_test(input_data)\n",
        "\n",
        "    #print(self.train_data)\n",
        "    #for line in input_data:\n",
        "    for line in self.train_data:\n",
        "      #print(line[0])\n",
        "      if line[1] in self.ClassCount:\n",
        "        self.ClassCount[line[1]] += 1\n",
        "      else :\n",
        "        self.ClassCount[line[1]] = float(1)\n",
        "      #tokenized_word = nltk.word_tokenize(line[0])\n",
        "      tokenized_word = self.tokenize_string(line[0],1)\n",
        "      #print(line[0])\n",
        "      #print(tokenized_word)\n",
        "      \n",
        "      #filtered_words = [word for word in tokenized_word if word not in stopwords.words('english')]\n",
        "      if use_stop_words :\n",
        "        tokenized_word = self.removing_stop_words(tokenized_word)\n",
        "      if use_stemming :\n",
        "        tokenized_word = self.stemming(tokenized_word)\n",
        "      fdist = nltk.FreqDist(tokenized_word)\n",
        "      #print(dict(fdist))\n",
        "      \n",
        "      #fdist = nltk.FreqDist(w.lower() for w in filtered_words) # stopwords in nltk in lowercase\n",
        "      #filtered_words = [word for word in fdist if word not in stopwords.words('english')]\n",
        "      for word in fdist:\n",
        "        self.ElementsByClass[line[1]] += fdist[word]\n",
        "        # add/update word to class\n",
        "        if word in self.WordsByClass[line[1]]:\n",
        "          self.WordsByClass[line[1]][word] += fdist[word]\n",
        "        else:\n",
        "          self.WordsByClass[line[1]][word] = float(fdist[word])\n",
        "        # add/update word to summary dict\n",
        "        if word in self.word_count_all :\n",
        "          self.word_count_all[word] += fdist[word]\n",
        "        else :\n",
        "          self.word_count_all[word] = float(fdist[word])\n",
        "    \n",
        "    # Prediction block (Predict, Evaluate, Calculate accuracy)\n",
        "    #accuracy_matrix = []\n",
        "    #for test_line in self.test_data:\n",
        "    #  prediction_probabilities = self.predict_one(test_line)\n",
        "    #  self.AccuracyPredictions.append(max(prediction_probabilities.items(), key=operator.itemgetter(1))[0])\n",
        "    #  self.AccuracyMatrix.append(test_line[1] == max(prediction_probabilities.items(), key=operator.itemgetter(1))[0])\n",
        "    #self.Accuracy = np.sum(self.AccuracyMatrix)/len(self.AccuracyMatrix)\n",
        "\n",
        "    #print(dict(fdist))\n",
        "    if print_word_distribution:\n",
        "      self.print_freq_distrinbution(self.word_count_all)\n",
        "    self.predict(self.test_data)\n",
        "    self.AccuracyPredictionsTest = copy.deepcopy(self.AccuracyPredictions)\n",
        "    self.AccuracyMatrixTest = copy.deepcopy(self.AccuracyMatrix)\n",
        "\n",
        "    return self.Accuracy\n",
        "\n",
        "  def print_errors(self, accuracy_matrix, predictions_matrix, source_data):\n",
        "    if accuracy_matrix : # check that it is not empty\n",
        "      print(\"Errors during training : \")\n",
        "      for i in range(len(accuracy_matrix)) :\n",
        "        print(\"Source class : {d} , Predicted class : {d}, Message : {s}\", source_data[i][1], predictions_matrix[i], source_data[i][0])\n",
        "\n",
        "  def print_training_errors(self):\n",
        "    self.print_errors(self, self.AccuracyMatrixTest, self.AccuracyPredictionsTest, self.test_data)\n",
        "\n",
        "  def predict(self, input_data) :\n",
        "    self.PredictionsProbabilities = []\n",
        "    self.AccuracyPredictions = []\n",
        "    self.AccuracyMatrix = []\n",
        "    for test_line in input_data:\n",
        "      prediction_probabilities = self.predict_one(test_line)\n",
        "      self.PredictionsProbabilities.append(prediction_probabilities)\n",
        "      self.AccuracyPredictions.append(max(prediction_probabilities.items(), key=operator.itemgetter(1))[0])\n",
        "      self.AccuracyMatrix.append(test_line[1] == max(prediction_probabilities.items(), key=operator.itemgetter(1))[0])\n",
        "    self.Accuracy = np.sum(self.AccuracyMatrix)/len(self.AccuracyMatrix)\n",
        "    if self.AccuracyPredictionsTest :\n",
        "      #print_errors(self.AccuracyMatrix, self.AccuracyPredictionsTest, input_data) # if this isn't called from FIT then print predictions\n",
        "      print(self.PredictionsProbabilities)\n",
        "\n",
        "  def predict_one(self, input_string) :\n",
        "    tokenized_test = self.tokenize_string(input_string[0],1)\n",
        "    if self.UseStopWords :\n",
        "      tokenized_test = self.removing_stop_words(tokenized_test)\n",
        "    if self.UseStemming :\n",
        "      tokenized_test = self.stemming(tokenized_test)\n",
        "    fdist_test = nltk.FreqDist(tokenized_test)\n",
        "    #tokenized_word=nltk.word_tokenize(new[0])\n",
        "    #fdist = nltk.FreqDist(tokenized_word)\n",
        "    #x = 0\n",
        "    for key in self.WordsByClass:\n",
        "      self.ClassProbabilityResult[key] = float(0) # set to 0 if there is previuos result \n",
        "      class_probability = self.ClassCount[key]/self.ClassCountAll\n",
        "      for word in fdist_test:\n",
        "        p1 = self.WordsByClass[key][word] if word in self.WordsByClass[key] else float(0)\n",
        "        p2 = self.ElementsByClass[key]\n",
        "        p3 = len(self.word_count_all)\n",
        "        class_probability *= pow(self.calculate_conditional_probability(p1,p2,p3),fdist_test[word])\n",
        "      self.ClassProbabilityResult[key] = math.log10(class_probability)\n",
        "    return self.ClassProbabilityResult\n",
        "\n",
        "  def print_freq_distrinbution(self, fdist):\n",
        "    # Frequency Distribution Plot\n",
        "    #FdistPrint = fdist.most_common(20) # take 20 most common words\n",
        "    #fdist.plot(30,cumulative=False)\n",
        "    #print(fdist)\n",
        "    lists = sorted(fdist.items())\n",
        "    x, y = zip(*lists)\n",
        "    plt.plot(x, y)\n",
        "    plt.show()\n",
        "\n",
        "  def removing_stop_words(self, tokenized_string) :\n",
        "    '''\n",
        "    Removing stop words from tokenized string (dict of words)\n",
        "\n",
        "    == Input ==\n",
        "\n",
        "    tokenized_string : dict of tokens (dictionary of words from splitted string)\n",
        "    \n",
        "    == Output ==  \n",
        "\n",
        "    filtered_tokens : dict of filtered words in lowercase\n",
        "    '''\n",
        "    filtered_tokens = []\n",
        "    for word in tokenized_string:\n",
        "      word = word.lower() # in case they arenet all lower cased\n",
        "      if word not in self.stopwords : #.words(\"english\"):\n",
        "        filtered_tokens.append(word)\n",
        "    #return [word for word in word_list if word not in self.stopwords.words('english')]\n",
        "    return filtered_tokens\n",
        "  \n",
        "  def stemming(self, tokenized_string) :\n",
        "    '''\n",
        "    Stemming\\n\n",
        "    Stemming is a process of linguistic normalization, which reduces words\n",
        "    to their word root word or chops off the derivational affixes.\n",
        "    For example, connection, connected, connecting word reduce to a common word \"connect\".\n",
        "\n",
        "    == Input ==\n",
        "\n",
        "    fdsit : dict of tokens (dictionary of words from splitted string)\n",
        "    \n",
        "    == Output ==  \n",
        "\n",
        "    stemmed_tokens : dict of preprocessed words\n",
        "    '''\n",
        "\n",
        "    ps = nltk.stem.PorterStemmer()\n",
        "\n",
        "    stemmed_words=[]\n",
        "    for w in tokenized_string:\n",
        "        stemmed_words.append(ps.stem(w))\n",
        "\n",
        "    return stemmed_words\n",
        "\n",
        "  def calculate_conditional_probability(self,wc,count_c,v):\n",
        "    '''\n",
        "    == Input ==\n",
        "\n",
        "    wc      : certain word count in appropriate class\\n\n",
        "    count_c : overall word count in appropriate class\\n\n",
        "    v       : unique word count in all classes\n",
        "    \n",
        "    == Output ==  \n",
        "    result : probability for certain word for certain class\n",
        "    '''\n",
        "    return (1 + wc)/(count_c + v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfX5VvAQj59_",
        "colab_type": "code",
        "outputId": "4ccf12bc-8f78-4a35-f209-c78ef7904d57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Test of sample data\n",
        "# Part 1\n",
        "# Copy test data from repo (do it once)\n",
        "!git clone https://github.com/MykhailoFokin/MachineLearning.git"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'MachineLearning'...\n",
            "remote: Enumerating objects: 27, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 148 (delta 14), reused 0 (delta 0), pack-reused 121\u001b[K\n",
            "Receiving objects: 100% (148/148), 39.65 MiB | 9.47 MiB/s, done.\n",
            "Resolving deltas: 100% (80/80), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXd6CrDqlBoi",
        "colab_type": "code",
        "outputId": "88202233-7c93-4ad3-ef37-cb121f4eb025",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        }
      },
      "source": [
        "# Test of sample data\n",
        "# Part 2\n",
        "source_data_list = []\n",
        "\n",
        "with open('MachineLearning/Naive_Bayes_data_test.csv', 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    for row in reader :\n",
        "      source_data_list.append(row)\n",
        "    #source_data_list = list(reader) # Train data\n",
        "\n",
        "# Create instance of NaiveBayes class\n",
        "nb = NaiveBayes()\n",
        "\n",
        "# Train our model\n",
        "# Tips: inside fit method it would be nice to split input data into train / test (80/20) sets and return model’ accuracy, e.g.:\n",
        "Accuracy = nb.fit(source_data_list, False, False, False, True)  # return accuracy\n",
        "print(\"Accuracy : {d} \", Accuracy) \n",
        "#Accuracy = nb.fit(source_data_list, True, False, False, True)  # return accuracy \n",
        "#print(\"Accuracy with stopwords : {d} \", Accuracy)\n",
        "#Accuracy = nb.fit(source_data_list, True, True, False, True)  # return accuracy \n",
        "#print(\"Accuracy with stopwords, stemming: {d} \", Accuracy)\n",
        "#Accuracy = nb.fit(source_data_list, True, True, True, True)  # return accuracy \n",
        "#print(\"Accuracy with stopwords, stemming, filtering out non alphanumeric symbols: {d} \", Accuracy)\n",
        "\n",
        "# Try to predict class of text\n",
        "nb.predict([\"Chinese Chinese Chinese Tokyo Japan\"])\n",
        "\n",
        "# Must return[ ('Chinese Chinese Chinese Tokyo Japan', '0')]\n",
        "# pobability {'1': 0.00013548070246744226, '0': 0.00030121377997263036}\n",
        "# or log     {'1': -7.906681345001262, '0': -7.10769031284391}\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7AAAAD5CAYAAAAeAtn7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-3d98fd20fe08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Train our model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Tips: inside fit method it would be nice to split input data into train / test (80/20) sets and return model’ accuracy, e.g.:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mAccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_data_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# return accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy : {d} \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#Accuracy = nb.fit(source_data_list, True, False, False, True)  # return accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-64-4f1f47199328>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, input_data, use_stop_words, use_stemming, use_alphanumeric, print_word_distribution)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprint_word_distribution\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_freq_distrinbution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_count_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAccuracyPredictionsTest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAccuracyPredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAccuracyMatrixTest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAccuracyMatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-64-4f1f47199328>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAccuracyMatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtest_line\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m       \u001b[0mprediction_probabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_line\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPredictionsProbabilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_probabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAccuracyPredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_probabilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-64-4f1f47199328>\u001b[0m in \u001b[0;36mpredict_one\u001b[0;34m(self, input_string)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mp3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_count_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mclass_probability\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_conditional_probability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfdist_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClassProbabilityResult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_probability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClassProbabilityResult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: math domain error"
          ]
        }
      ]
    }
  ]
}