{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naive Bayes text classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOyyTGyWSL/Y/idC69ZAV9m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MykhailoFokin/MachineLearning/blob/master/Naive_Bayes_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgyucsrLYHC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Basic test of algorithm (clear nltk with defaults)\n",
        "# Move further to the next block for full implementation\n",
        "from collections import Counter\n",
        "import itertools\n",
        "import math\n",
        "import nltk\n",
        "import copy\n",
        "# nltk.download(\"popular\")\n",
        "\n",
        "# Train data\n",
        "data = [[\"Chinese Beijing Chinese\",\"0\"],\n",
        "        [\"Chinese Chinese Shanghai\",\"0\"], \n",
        "        [\"Chinese Macao\",\"0\"],\n",
        "        [\"Tokyo Japan Chinese\",\"1\"]]\n",
        "\n",
        "class_count = {}\n",
        "class_count_all = float(len(data))\n",
        "word_count_all = {}\n",
        "class_words_count = {}\n",
        "wc_by_class = {item[1]: {} for item in data}\n",
        "class_probability_template = copy.deepcopy(wc_by_class)\n",
        "class_words_count = {item[1]: float(0) for item in data}\n",
        "for line in data:\n",
        "  if line[1] in class_count:\n",
        "    class_count[line[1]] += 1\n",
        "  else :\n",
        "    class_count[line[1]] = float(1)\n",
        "  tokenized_word=nltk.word_tokenize(line[0])\n",
        "  fdist = nltk.FreqDist(tokenized_word)\n",
        "  #fdist = nltk.FreqDist(w.lower() for w in tokenized_word)\n",
        "  for word in fdist:\n",
        "    #words_all += 1   # increase words counter\n",
        "    class_words_count[line[1]] += fdist[word]\n",
        "    # add/update word to class\n",
        "    if word in wc_by_class[line[1]]:\n",
        "      wc_by_class[line[1]][word] += fdist[word]\n",
        "    else:\n",
        "      wc_by_class[line[1]][word] = float(fdist[word])\n",
        "    # add/update word to summary dict\n",
        "    if word in word_count_all :\n",
        "      word_count_all[word] += fdist[word]\n",
        "    else :\n",
        "      word_count_all[word] = float(fdist[word])\n",
        "  #tokenized_words.append((dict(fdist), {\"class\" : line[1]}))\n",
        "\n",
        "def calculate_conditional_probability(wc,count_c,v):\n",
        "  '''\n",
        "  == Input ==\n",
        "\n",
        "  wc      : certain word count in appropriate class\\n\n",
        "  count_c : overall word count in appropriate class\\n\n",
        "  v       : unique word count in all classes\n",
        "  \n",
        "  == Output ==  \n",
        "  result : probability for certain word for certain class\n",
        "  '''\n",
        "  return (1 + wc)/(count_c + v)\n",
        "\n",
        "# Test\n",
        "new = [\"Chinese Chinese Chinese Tokyo Japan\"]\n",
        "\n",
        "tokenized_word=nltk.word_tokenize(new[0])\n",
        "fdist = nltk.FreqDist(tokenized_word)\n",
        "#print(Counter(tok['KeyName'] for tok in fdist))\n",
        "x = 0\n",
        "for key in wc_by_class:\n",
        "  #print(\"Key of main dictionary: \" + key)\n",
        "  class_probability = class_count[key]/class_count_all\n",
        "  for word in fdist:\n",
        "    p1 = wc_by_class[key][word] if word in wc_by_class[key] else float(0)\n",
        "    p2 = class_words_count[key]\n",
        "    p3 = len(word_count_all)\n",
        "    class_probability *= pow(calculate_conditional_probability(p1,p2,p3),fdist[word])\n",
        "  #print(\"Class\", key, class_probability)\n",
        "  class_probability_template[key] = class_probability\n",
        "\n",
        "print(class_probability_template)\n",
        "print(\"Log10 : {d}\", {k: math.log10(v) for k, v in class_probability_template.items()})\n",
        "print(\"Log : {d}\", {k: math.log(v) for k, v in class_probability_template.items()})\n",
        "print(\"Log2 : {d}\", {k: math.log2(v) for k, v in class_probability_template.items()})\n",
        "print(\"Log1p : {d}\", {k: math.log1p(v) for k, v in class_probability_template.items()})\n",
        "print(\"Exp : {d}\", {k: math.exp(v) for k, v in class_probability_template.items()})\n",
        "print(\"Expm1 : {d}\", {k: math.expm1(v) for k, v in class_probability_template.items()})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVQLevCbeY0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NLTK preparation\n",
        "# Run it to get basic library content for futher usage\n",
        "import nltk\n",
        "nltk.download(\"popular\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fekOwd4L6aAT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import block\n",
        "import nltk\n",
        "#from collections import Counter\n",
        "import itertools\n",
        "import math\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "from stop_words import get_stop_words\n",
        "import numpy as np\n",
        "\n",
        "# Define block\n",
        "class NaiveBayes :\n",
        "\n",
        "  def __init__ (self) :\n",
        "    self.ClassCount = {}\n",
        "    word_count_all = {}\n",
        "    self.ElementsByClass = {}  #   class_words_count = {}\n",
        "    #wc_by_class = {item[1]: {} for item in data}\n",
        "    # from stop_words import get_stop_words \n",
        "    # stop_words = list(get_stop_words('en'))  # About 900 words\n",
        "    # from nltk.corpus import stopwords\n",
        "    # list(stopwords.words('english')) # About 150 stopwords\n",
        "    # stop_words.extend(nltk_words)\n",
        "    self.stopwords = set(nltk.corpus.stopwords.words(\"english\")) # import stopwords and remove duplicates\n",
        "    self.AccuracyMatrix = []\n",
        "    self.Accuracy = float(0)\n",
        "    self.AccuracyPredictions = []\n",
        "    self.PredictionsProbabilities = []\n",
        "\n",
        "  def change_stopwords_list(self, whatlist = 'Default') :\n",
        "    '''\n",
        "    Optionally we can change default Enlgish stop words from NLTK\n",
        "\n",
        "    == Input ==\n",
        "\n",
        "    whatlist : string parameter for certain stop words list\\n\n",
        "       Default : Use only NLTK list. About 150 stopwords\n",
        "       Alternative : Use stop_words library. About 900 words\n",
        "       Extended : Use both together\n",
        "    '''\n",
        "    if whatlist == 'Default' :\n",
        "      self.stopwords = set(nltk.corpus.stopwords.words(\"english\")) # import stopwords and remove duplicates\n",
        "    elif whatlist == 'Alternative' :\n",
        "      self.stopwords = set(get_stop_words('en')) # import stopwords and remove duplicates\n",
        "    elif whatlist == 'Extended' :\n",
        "      nltkstopwords = list(nltk.corpus.stopwords.words(\"english\"))\n",
        "      stop_words = list(get_stop_words('en'))\n",
        "      stop_words.extend(nltk_words)\n",
        "      self.stopwords = set(stop_words)\n",
        "\n",
        "  def tokenize_string(self, input_string, tokenizer_type=0):\n",
        "    '''\n",
        "    Split string into tokens.\n",
        "\n",
        "    == Input ==\n",
        "\n",
        "    input_string : string parameter for certain stop words list\\n\n",
        "    tokenizer_type : how split string into words, what tokenizer rules should it use\\n\n",
        "       0 : Default value, it will use default delimiters with function nltk.word_tokenize\\n\n",
        "       1 : It will picks out sequences of alphanumeric characters as tokens and drops everything else.\\n\n",
        "           nltk.tokenize.RegexpTokenizer function\n",
        "    '''\n",
        "    if tokenizer_type == 1:\n",
        "      tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
        "      tokenized_string = tokenizer.tokenize('Eighty-seven miles to go, yet.  Onward!')\n",
        "    else :\n",
        "      tokenized_string = nltk.word_tokenize(input_string)\n",
        "    return tokenized_string\n",
        "\n",
        "  def fit(self, input_data, use_stop_words, use_stemming, use_alphanumeric) :\n",
        "    '''\n",
        "    Takes input_data and apply on it Naive Bayes algorithm.\\n\n",
        "    Input will be separated depends on how many elements in the list,\\n\n",
        "    not more than 2,000 elements will be taken as a test list.\\n\n",
        "    The result of this method is accuracy of predictions based on splitting\\n\n",
        "    Input dataset into train and test and applying algorithm on it.\n",
        "\n",
        "    == Input ==\n",
        "\n",
        "    input_data : string parameter for certain stop words list\\n\n",
        "    use_stop_words   : (False,True) Should be apply filtering out stopwords\n",
        "    use_stemming     : (False,True) Should it also uses lexicon(linguistic) normalization\n",
        "    use_alphanumeric : (False,True) Should non alphanumeric symbols be excluded\n",
        "    '''\n",
        "    #self.ClassCount = \n",
        "    self.ClassCountAll = float(len(input_data))  # class_count_all = float(len(data))\n",
        "    self.ElementsByClass = {item[1]: float(0) for item in data}\n",
        "    self.WordsByClass = {item[1]: {} for item in data}\n",
        "    self.ClassProbabilityResult = copy.deepcopy(self.WordsByClass)\n",
        "    self.UseStopWords = use_stop_words\n",
        "    self.UseStemming = use_stemming\n",
        "    self.AlphaNumeric = 1 if use_alphanumeric else 0\n",
        "    self.AccuracyMatrix = []\n",
        "    self.AccuracyPredictions = []\n",
        "\n",
        "    for line in input_data:\n",
        "      if line[1] in self.ClassCount:\n",
        "        self.ClassCount[line[1]] += 1\n",
        "      else :\n",
        "        self.ClassCount[line[1]] = float(1)\n",
        "      #tokenized_word = nltk.word_tokenize(line[0])\n",
        "      tokenized_word = tokenize_string(line[0],1)\n",
        "      \n",
        "      #filtered_words = [word for word in tokenized_word if word not in stopwords.words('english')]\n",
        "      if use_stop_words :\n",
        "        tokenized_word = removing_stop_words(tokenized_word)\n",
        "      if use_stemming :\n",
        "        tokenized_word = stemming(tokenized_word)\n",
        "      fdist = nltk.FreqDist(tokenized_word)\n",
        "      #fdist = nltk.FreqDist(w.lower() for w in filtered_words) # stopwords in nltk in lowercase\n",
        "      #filtered_words = [word for word in fdist if word not in stopwords.words('english')]\n",
        "    for word in fdist:\n",
        "      self.ElementsByClass[line[1]] += fdist[word]\n",
        "      # add/update word to class\n",
        "      if word in self.WordsByClass[line[1]]:\n",
        "        self.WordsByClass[line[1]][word] += fdist[word]\n",
        "      else:\n",
        "        self.WordsByClass[line[1]][word] = float(fdist[word])\n",
        "      # add/update word to summary dict\n",
        "      if word in word_count_all :\n",
        "        word_count_all[word] += fdist[word]\n",
        "      else :\n",
        "        word_count_all[word] = float(fdist[word])\n",
        "    \n",
        "    # Prediction block (Predict, Evaluate, Calculate accuracy)\n",
        "    #accuracy_matrix = []\n",
        "    for test_line in self.test_data:\n",
        "      prediction_probabilities = predict(test_line)\n",
        "      self.AccuracyPredictions.append(max(prediction_probabilities.items(), key=operator.itemgetter(1))[0])\n",
        "      self.AccuracyMatrix.append(test_line[1] == max(prediction_probabilities.items(), key=operator.itemgetter(1))[0])\n",
        "    self.Accuracy = np.sum(self.AccuracyMatrix)/len(self.AccuracyMatrix)\n",
        "\n",
        "    predict(self.test_data)\n",
        "    self.AccuracyPredictionsTest = copy.deepcopy(self.AccuracyPredictions)\n",
        "    self.AccuracyMatrixTest = copy.deepcopy(self.AccuracyMatrix)\n",
        "\n",
        "  def print_errors(self, accuracy_matrix, predictions_matrix, source_data):\n",
        "    if accuracy_matrix : # check that it is not empty\n",
        "      print(\"Errors during training : \")\n",
        "      for i in range(len(accuracy_matrix)) :\n",
        "        print(\"Source class : {d} , Predicted class : {d}, Message : {s}\", source_data[i][1], predictions_matrix[i], source_data[i][0])\n",
        "\n",
        "  def print_training_errors(self):\n",
        "    print_errors(self, self.AccuracyMatrixTest, self.AccuracyPredictionsTest, self.test_data)\n",
        "\n",
        "  def predict(self, input_data) :\n",
        "    self.PredictionsProbabilities = []\n",
        "    self.AccuracyPredictions = []\n",
        "    self.AccuracyMatrix = []\n",
        "    for test_line in input_data:\n",
        "      prediction_probabilities = predict(test_line)\n",
        "      self.PredictionsProbabilities.append(prediction_probabilities)\n",
        "      self.AccuracyPredictions.append(max(prediction_probabilities.items(), key=operator.itemgetter(1))[0])\n",
        "      self.AccuracyMatrix.append(test_line[1] == max(prediction_probabilities.items(), key=operator.itemgetter(1))[0])\n",
        "    #self.Accuracy = np.sum(self.AccuracyMatrix)/len(self.AccuracyMatrix)\n",
        "    if self.AccuracyPredictionsTest \n",
        "      #print_errors(self.AccuracyMatrix, self.AccuracyPredictionsTest, input_data) # if this isn't called from FIT then print predictions\n",
        "      print()\n",
        "\n",
        "  def predict_one(self, input_string) :\n",
        "    tokenized_test = tokenize_string(input_string[0],1)\n",
        "    if self.UseStopWords :\n",
        "      tokenized_test = removing_stop_words(tokenized_test)\n",
        "    if self.UseStemming :\n",
        "      tokenized_test = stemming(tokenized_test)\n",
        "    fdist_test = nltk.FreqDist(tokenized_test)\n",
        "    #tokenized_word=nltk.word_tokenize(new[0])\n",
        "    #fdist = nltk.FreqDist(tokenized_word)\n",
        "    #x = 0\n",
        "    for key in self.WordsByClass:\n",
        "      self.ClassProbabilityResult[key] = float(0) # set to 0 if there is previuos result \n",
        "      class_probability = self.ClassCount[key]/self.ClassCountAll\n",
        "      for word in fdist_test:\n",
        "        p1 = self.WordsByClass[key][word] if word in self.WordsByClass[key] else float(0)\n",
        "        p2 = self.ElementsByClass[key]\n",
        "        p3 = len(word_count_all)\n",
        "        class_probability *= pow(calculate_conditional_probability(p1,p2,p3),fdist_test[word])\n",
        "      self.ClassProbabilityResult[key] = class_probability\n",
        "    return self.ClassProbabilityResult\n",
        "\n",
        "  def print_freq_distrinbution(self, fdist):\n",
        "    # Frequency Distribution Plot\n",
        "    #FdistPrint = fdist.most_common(20) # take 20 most common words\n",
        "    fdist.plot(30,cumulative=False)\n",
        "    plt.show()\n",
        "\n",
        "  def removing_stop_words(self, tokenized_string) :\n",
        "     '''\n",
        "    Removing stop words from tokenized string (dict of words)\n",
        "\n",
        "    == Input ==\n",
        "\n",
        "    tokenized_string : dict of tokens (dictionary of words from splitted string)\n",
        "    \n",
        "    == Output ==  \n",
        "\n",
        "    filtered_tokens : dict of filtered words in lowercase\n",
        "    '''   \n",
        "    filtered_tokens = []\n",
        "    for word in tokenized_string:\n",
        "        word = word.lower() # in case they arenet all lower cased\n",
        "        if word not in self.stopwords.words(\"english\"):\n",
        "            filtered_tokens.append(word)\n",
        "    return filtered_tokens\n",
        "    #return [word for word in word_list if word not in self.stopwords.words('english')]\n",
        "  \n",
        "  def stemming(self, tokenized_string) :\n",
        "    '''\n",
        "    Stemming\\n\n",
        "    Stemming is a process of linguistic normalization, which reduces words\n",
        "    to their word root word or chops off the derivational affixes.\n",
        "    For example, connection, connected, connecting word reduce to a common word \"connect\".\n",
        "\n",
        "    == Input ==\n",
        "\n",
        "    fdsit : dict of tokens (dictionary of words from splitted string)\n",
        "    \n",
        "    == Output ==  \n",
        "\n",
        "    stemmed_tokens : dict of preprocessed words\n",
        "    '''\n",
        "\n",
        "    ps = nltk.stem.PorterStemmer()\n",
        "\n",
        "    stemmed_words=[]\n",
        "    for w in tokenized_string:\n",
        "        stemmed_words.append(ps.stem(w))\n",
        "\n",
        "    return stemmed_words"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}