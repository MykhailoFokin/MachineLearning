{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naive Bayes text classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNoZXxVVlalOevyBxEZyHyA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MykhailoFokin/MachineLearning/blob/master/Naive_Bayes_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fekOwd4L6aAT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import block\n",
        "import nltk\n",
        "from collections import Counter\n",
        "\n",
        "# Define block\n",
        "class NaiveBayes :\n",
        "\n",
        "  def fit(self, input_data) :\n",
        "    self.classCount = len(input_data)\n",
        "    self.CountByClass = Counter(tok['KeyName'] for tok in tokens)\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgyucsrLYHC4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "dbc33435-09aa-49cd-8508-be7c466b791f"
      },
      "source": [
        "from collections import Counter\n",
        "import itertools\n",
        "# Train data\n",
        "data = [[\"Chinese Beijing Chinese\",\"0\"],\n",
        "        [\"Chinese Chinese Shanghai\",\"0\"], \n",
        "        [\"Chinese Macao\",\"0\"],\n",
        "        [\"Tokyo Japan Chinese\",\"1\"]]\n",
        "\n",
        "# print [(k, ' '.join(v)) for k, v in data.items()]\n",
        "\n",
        "# print(list(itertools.chain.from_iterable(data)))\n",
        "# ['Chinese Beijing Chinese', '0', 'Chinese Chinese Shanghai', '0', 'Chinese Macao', '0', 'Tokyo Japan Chinese', '1']\n",
        "\n",
        "#itemDict = {item[1]: {} for item in data}\n",
        "\n",
        "#print(itemDict)\n",
        "\n",
        "import nltk\n",
        "class_count = {}\n",
        "# nltk.download(\"popular\")\n",
        "# main_dict = dict()\n",
        "dd = {item[1]: {} for item in data}\n",
        "tokenized_words = []\n",
        "for line in data:\n",
        "  if line[1] in class_count:\n",
        "    class_count[line[1]] += 1\n",
        "  else :\n",
        "    class_count[line[1]] = 1\n",
        "  tokenized_word=nltk.word_tokenize(line[0])\n",
        "  #print(tokenized_word)\n",
        "  # main_dict(\"class\":line[1] : tokenized_word)\n",
        "  fdist = nltk.FreqDist(tokenized_word)\n",
        "  #fdist = nltk.FreqDist(w.lower() for w in tokenized_word)\n",
        "  for word in fdist:\n",
        "    if word in dd[line[1]]:\n",
        "      dd[line[1]][word] += fdist[word]\n",
        "    else:\n",
        "      dd[line[1]][word] = fdist[word]\n",
        "  tokenized_words.append((dict(fdist), {\"class\" : line[1]}))\n",
        "\n",
        "# from nltk.probability import FreqDist\n",
        "#fdist = nltk.FreqDist(tokenized_word)\n",
        "#print(fdist)\n",
        "\n",
        "\n",
        "#print(tokenized_words)\n",
        "\n",
        "#fdist.most_common(2)\n",
        "\n",
        "print(dd)\n",
        "print(class_count)\n",
        "\n",
        "new = [\"Chinese Chinese Chinese Tokyo Japan\"]\n",
        "\n",
        "tokenized_word=nltk.word_tokenize(new[0])\n",
        "fdist = nltk.FreqDist(tokenized_word)\n",
        "x = 0\n",
        "for key in dd:\n",
        "   print(key)\n",
        "   for word in fdist:\n",
        "     if word in dd[key]:\n",
        "       y="
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'0': {'Chinese': 5, 'Beijing': 1, 'Shanghai': 1, 'Macao': 1}, '1': {'Tokyo': 1, 'Japan': 1, 'Chinese': 1}}\n",
            "{'0': 3, '1': 1}\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}