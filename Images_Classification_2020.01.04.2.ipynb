{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Images_Classification_2020.01.03.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MykhailoFokin/MachineLearning/blob/master/Images_Classification_2020.01.04.2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyPNlcb6nKgy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import fnmatch\n",
        "import numpy as np\n",
        "import json\n",
        "import scipy\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "from sklearn.metrics import median_absolute_error\n",
        "from sklearn.metrics import explained_variance_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from skimage import feature"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVOSURUpYlBh",
        "colab_type": "code",
        "outputId": "9e4b5770-009b-4ffe-83d7-ce5cc0d2b164",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Copy images(photos) from repo to colab\n",
        "!git clone https://github.com/SilvesterHsu/ORLFaceRecognition-PCA.git\n",
        "\n",
        "#Copy eyes data from previous processing\n",
        "!git clone https://github.com/MykhailoFokin/MachineLearning.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'ORLFaceRecognition-PCA' already exists and is not an empty directory.\n",
            "fatal: destination path 'MachineLearning' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljk0ib0q9MAS",
        "colab_type": "code",
        "outputId": "7fba4e4c-3957-4671-9b4b-c1a1fcc1c446",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Class of model (train and evaluate)\n",
        "class manyAlgorithmsModel:\n",
        "\n",
        "  def __init__(self, trainMatrix, testMatrix, trainLabels, testLabels, dataset_shuffled,**kwargs):\n",
        "    self.estimations_mse = []\n",
        "    self.estimations_mae = []\n",
        "    #self.estimations_msle = []\n",
        "    #self.estimations_std = []\n",
        "\n",
        "    self.train_images = [] # this data structure is used for print (if required, set print_difference_photos = True)\n",
        "\n",
        "    self.dataset_shuffle = dataset_shuffled\n",
        "    #self.print_distribution = print_distribution\n",
        "    #self.print_unpredicted_data = print_unpredicted_data\n",
        "    self.print_chiSuared_data = True\n",
        "\n",
        "    self.testMatrix = testMatrix\n",
        "    self.trainMatrix = trainMatrix\n",
        "    self.test_labels = testLabels\n",
        "    self.train_labels = trainLabels\n",
        "\n",
        "    if \"originalTestMatrix\" in kwargs:\n",
        "      self.originalTestMatrix = kwargs[\"originalTestMatrix\"]\n",
        "    if \"originalTrainMatrix\" in kwargs:\n",
        "      self.originalTrainMatrix = kwargs[\"originalTrainMatrix\"]\n",
        "    if \"numberOfPoints\" in kwargs:\n",
        "      self.numberOfPoints = kwargs[\"numberOfPoints\"]\n",
        "\n",
        "  def predict(self, print_distribution) :\n",
        "\n",
        "    #self.testMatrix  = self.flatten_matrices(self.testMatrix)\n",
        "    #self.trainMatrix = self.flatten_matrices(self.trainMatrix)\n",
        "\n",
        "    self.estimations_mse = []\n",
        "    self.estimations_mae = []\n",
        "    #self.estimations_msle = []\n",
        "    #self.estimations_std = []\n",
        "    \n",
        "    self.train_images = []\n",
        "\n",
        "    for test_image in self.testMatrix:\n",
        "\n",
        "      matrix_mse = []\n",
        "      matrix_mae = []\n",
        "      #matrix_msle = []\n",
        "      #matrix_std = []\n",
        "\n",
        "      for train_image in self.trainMatrix:\n",
        "\n",
        "        matrix_mse.append(mean_squared_error(test_image, train_image))\n",
        "        matrix_mae.append(mean_absolute_error(test_image, train_image))\n",
        "        #matrix_msle.append(mean_squared_log_error(test_image, train_image))\n",
        "        #matrix_std.append(np.sqrt(mean_squared_error(test_image, train_image)))\n",
        "\n",
        "      self.estimations_mse.append(self.train_labels[np.argmin(matrix_mse)])\n",
        "      self.estimations_mae.append(self.train_labels[np.argmin(matrix_mae)])\n",
        "      #self.estimations_msle.append(self.train_labels[np.argmin(matrix_msle)])\n",
        "      #self.estimations_std.append(self.train_labels[np.argmin(matrix_std)])\n",
        "\n",
        "      self.train_images.append(self.trainMatrix[np.argmin(matrix_mse)]) # for print\n",
        "\n",
        "    # print dataset distribution\n",
        "    if print_distribution:\n",
        "      plt.hist(self.test_labels, bins=range(1, len(list(set(self.test_labels))) + 2))\n",
        "      plt.yticks(np.arange(0, len(list(set(img_list))) + 1, 1))\n",
        "      plt.show()\n",
        "\n",
        "  def predict_chiSquared(self, print_distribution) :\n",
        "    \n",
        "    self.estimations_chiSquared = []\n",
        "    self.train_images_chiSquared = []\n",
        "\n",
        "    for test_image in self.testMatrix:\n",
        "      matrix_chiSquared = []\n",
        "\n",
        "      for train_image in self.trainMatrix:\n",
        "        matrix_chiSquared.append(chiSquared(test_image, train_image))\n",
        "\n",
        "      #print(matrix_chiSquared)\n",
        "      self.estimations_chiSquared.append(self.train_labels[np.argmin(matrix_chiSquared)])\n",
        "      self.train_images_chiSquared.append(self.originalTrainMatrix[np.argmin(matrix_chiSquared)]) # for print\n",
        "\n",
        "    # print dataset distribution\n",
        "    if print_distribution:\n",
        "      plt.hist(self.test_labels, bins=range(1, len(list(set(self.test_labels))) + 2))\n",
        "      plt.yticks(np.arange(0, len(list(set(img_list))) + 1, 1))\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "\n",
        "  def evaluate(self) :\n",
        "\n",
        "    self.evaluation_mse = 0\n",
        "    self.evaluation_mae = 0\n",
        "    #self.evaluation_msle = 0\n",
        "    #self.evaluation_std = 0\n",
        "\n",
        "    if self.estimations_mse :\n",
        "\n",
        "      self.result_mse = np.equal(self.test_labels, self.estimations_mse)\n",
        "      self.result_mae = np.equal(self.test_labels, self.estimations_mae)\n",
        "      #self.result_msle = np.equal(self.test_labels, self.estimations_msle)\n",
        "      #self.result_std = np.equal(self.test_labels, self.estimations_std)\n",
        "\n",
        "      matrix_length = len(self.result_mse)\n",
        "\n",
        "      self.evaluation_mse = np.sum(self.result_mse)/matrix_length\n",
        "      self.evaluation_mae = np.sum(self.result_mae)/matrix_length\n",
        "      #self.evaluation_msle = np.sum(self.result_msle)/matrix_length\n",
        "      #self.evaluation_std = np.sum(self.result_std)/matrix_length\n",
        "\n",
        "      print('MSE{:s}.  Train/Test : {:d} / {:d} - {:.3f}'.format(' with Shuffle' if self.dataset_shuffle else '',100-split_k*10, split_k*10, self.evaluation_mse))\n",
        "      print('MAE{:s}.  Train/Test : {:d} / {:d} - {:.3f}'.format(' with Shuffle' if self.dataset_shuffle else '',100-split_k*10, split_k*10, self.evaluation_mae))\n",
        "      #print('MSLE{:s}.  Train/Test : {:d} / {:d} - {:.3f}'.format(' with Shuffle' if self.dataset_shuffle else '',100-split_k*10, split_k*10, self.evaluation_msle))\n",
        "      #print('STD{:s}.  Train/Test : {:d} / {:d} - {:.3f}'.format(' with Shuffle' if self.dataset_shuffle else '',100-split_k*10, split_k*10, self.evaluation_std))\n",
        "\n",
        "    return self.evaluation_mse, self.evaluation_mae\n",
        "\n",
        "  def evaluate_chiSquared(self) :\n",
        "    self.evaluation_chiSquared = 0\n",
        "\n",
        "    if self.estimations_chiSquared :\n",
        "      self.result_chiSquared = np.equal(self.test_labels, self.estimations_chiSquared)\n",
        "      matrix_length = len(self.result_chiSquared)\n",
        "      self.evaluation_chiSquared = np.sum(self.result_chiSquared)/matrix_length\n",
        "\n",
        "      print('ChiSquared{:s}.  Train/Test : {:d} / {:d} - {:.3f}'.format(' with Shuffle' if self.dataset_shuffle else '',100-split_k*10, split_k*10, self.evaluation_chiSquared))\n",
        "\n",
        "    return self.evaluation_chiSquared\n",
        "\n",
        "  def print_data_errors(self) :\n",
        "    if len(self.result_mse) > 0 :\n",
        "      for img_ind in range(1,len(self.result_mse)):\n",
        "        if self.result_mse[img_ind]==False:\n",
        "          img_src = self.testMatrix[img_ind].reshape(112,92)\n",
        "          print(\"Index {0}, test:\", img_ind)\n",
        "          print(\"Index {0}, train:\", img_ind)\n",
        "          img_train = self.train_images[img_ind].reshape(112,92)\n",
        "          plt.subplot(1,2,1),plt.imshow(img_src)\n",
        "          plt.title(\"Test photo\")\n",
        "          plt.subplot(1,2,2),plt.imshow(img_train)\n",
        "          plt.title(\"Train photo\")\n",
        "          plt.show()\n",
        "\n",
        "  def print_LBP_errors(self) :\n",
        "      if self.print_chiSuared_data :\n",
        "        for img_ind in range(1,len(self.result_chiSquared)):\n",
        "          if self.result_chiSquared[img_ind]==False:\n",
        "            print(\"Index {0} :\".format(img_ind))\n",
        "            img_src = self.originalTestMatrix[img_ind].reshape(112,92)\n",
        "            #src_img_train = self.originalTrainMatrix[img_ind].reshape(112,92)\n",
        "            #src_img_test = self.originalTestMatrix[img_ind].reshape(112,92)\n",
        "            img_train = self.train_images_chiSquared[img_ind].reshape(112,92)\n",
        "            src_img_train = getLBPimage(self.train_images_chiSquared[img_ind],self.numberOfPoints)\n",
        "            src_img_test = getLBPimage(self.originalTestMatrix[img_ind],self.numberOfPoints)\n",
        "\n",
        "            plt.subplot(1,4,1),plt.imshow(img_src)\n",
        "            plt.title(\"Test photo\")\n",
        "            plt.subplot(1,4,2),plt.imshow(img_train)\n",
        "            plt.title(\"Train photo\")\n",
        "            plt.subplot(1,4,3),plt.imshow(src_img_test)\n",
        "            plt.title(\"Original test\")\n",
        "            plt.subplot(1,4,4),plt.imshow(src_img_train)\n",
        "            plt.title(\"Original train\")\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "def angle_by_eyes(eyes_points) :\n",
        "  \n",
        "  #return np.arctan((eyes_points['left_eye_y'] - eyes_points['right_eye_y']) / (eyes_points['left_eye_x'] - eyes_points['right_eye_x']))\n",
        "  return math.atan2(eyes_points['left_eye_y'] - eyes_points['right_eye_y'], eyes_points['left_eye_x'] - eyes_points['right_eye_x'])\n",
        "  #return 10\n",
        "\n",
        "\n",
        "def align_images_by_eyes(p_img_matrix, p_eye_data, p_fulllabels) :\n",
        "  '''\n",
        "  Rotate images to be align with eye angle\n",
        "  '''\n",
        "\n",
        "  rotated_matrix = []\n",
        "\n",
        "  for eye_data in p_eye_data :\n",
        "    angle = angle_by_eyes(eye_data)\n",
        "    current_label = eye_data[\"img\"].rstrip('.pgm')\n",
        "    img_index, = np.where(p_fulllabels == current_label)[0]\n",
        "    \n",
        "    if img_index > 0:\n",
        "      #rotated_matrix.append(scipy.ndimage.rotate(p_img_matrix[img_index], angle, reshape = False))\n",
        "      p_img_matrix[img_index] = scipy.ndimage.rotate(p_img_matrix[img_index], angle, reshape = False)\n",
        "    #else :\n",
        "      #rotated_matrix.append(p_img_matrix[img_index])\n",
        "\n",
        "  return p_img_matrix\n",
        "\n",
        "# Define block of different types of normalization\n",
        "def f_0_1(a,x_min,x_max):\n",
        "  return (a-x_min)/(x_max-x_min)\n",
        "\n",
        "vectorized_f_0_1 = np.vectorize(f_0_1)\n",
        "\n",
        "def f__1_1(a,x_min,x_max):\n",
        "  return 2*((a-x_min)/(x_max-x_min))-1\n",
        "\n",
        "vectorized_f__1_1 = np.vectorize(f__1_1)\n",
        "\n",
        "def f_std(a,x_mean,x_std):\n",
        "  return (a-x_mean)/x_std\n",
        "\n",
        "vectorized_f_std = np.vectorize(f_std)\n",
        "\n",
        "def f_min_max_0_1(a):\n",
        "  return vectorized_f_0_1(a,a.min(),a.max())\n",
        "\n",
        "def f_min_max__1_1(a):\n",
        "  return vectorized_f__1_1(a,a.min(),a.max())\n",
        "\n",
        "def f_mean_std(a):\n",
        "  return vectorized_f__1_1(a,np.mean(a),np.std(a))\n",
        "\n",
        "# Сlass of LBP realization \n",
        "class LocalBinaryPatterns:\n",
        "  def __init__(self, numPoints, radius):\n",
        "    # store the number of points and radius\n",
        "    self.numPoints = numPoints\n",
        "    self.radius = radius\n",
        "  \n",
        "  def describe(self, image, eps=1e-7):\n",
        "    # compute the Local Binary Pattern representation\n",
        "    # of the image, and then use the LBP representation\n",
        "    # to build the histogram of patterns\n",
        "    lbp = feature.local_binary_pattern(image, self.numPoints, self.radius, method=\"uniform\")\n",
        "    #print(lbp)\n",
        "    #(hist, _) = np.histogram(lbp.ravel(), bins=np.arange(0, self.numPoints + 3), range=(0, self.numPoints + 2))\n",
        "    #(hist, _) = np.histogram(lbp,bins=np.arange(0, self.numPoints + 3), range=(0, self.numPoints + 2))\n",
        "\n",
        "    vertical = int(lbp.shape[0] / self.numPoints)\n",
        "    horizontal = int(lbp.shape[1] / self.numPoints)\n",
        "\n",
        "    histograms = []\n",
        "    for r in range(0, lbp.shape[0] - (vertical + 1), vertical):\n",
        "        for c in range(0, lbp.shape[1] - (horizontal + 1), horizontal):\n",
        "            #histograms.append(np.histogram(window, bins=np.arange(0, self.numPoints + 3), range=(0, self.numPoints + 2))[0])\n",
        "            histogram = np.histogram(lbp[r:r+vertical,c:c+horizontal], bins=np.arange(0, self.numPoints + 3))[0]\n",
        "            histogram = histogram.astype(\"float\")\n",
        "            histograms.append(histogram.flatten())\n",
        "    \n",
        "    # normalize the histogram\n",
        "    #hist = hist.astype(\"float\")\n",
        "    #hist /= (hist.sum() + eps)\n",
        "    \n",
        "    # return the histogram of Local Binary Patterns\n",
        "    return np.array(histograms)\n",
        "\n",
        "def getLBPimage(image,numPoints):\n",
        "  '''\n",
        "  == Input ==\n",
        "  image  : color image of shape (height, width)\n",
        "  \n",
        "  == Output ==  \n",
        "  imgLBP : LBP converted image of the same shape as \n",
        "  '''\n",
        "  \n",
        "  ### Step 0: Step 0: Convert an image to grayscale\n",
        "  imgLBP = np.zeros_like(image)\n",
        "  neighboor = numPoints \n",
        "  for ih in range(0,image.shape[0] - neighboor):\n",
        "      for iw in range(0,image.shape[1] - neighboor):\n",
        "          ### Step 1: 3 by 3 pixel\n",
        "          img          = image[ih:ih+neighboor,iw:iw+neighboor]\n",
        "          center       = img[1,1]\n",
        "          img01        = (img >= center)*1.0\n",
        "          img01_vector = img01.T.flatten()\n",
        "          # it is ok to order counterclock manner\n",
        "          # img01_vector = img01.flatten()\n",
        "          ### Step 2: **Binary operation**:\n",
        "          img01_vector = np.delete(img01_vector,4)\n",
        "          ### Step 3: Decimal: Convert the binary operated values to a digit.\n",
        "          where_img01_vector = np.where(img01_vector)[0]\n",
        "          if len(where_img01_vector) >= 1:\n",
        "              num = np.sum(2**where_img01_vector)\n",
        "          else:\n",
        "              num = 0\n",
        "          imgLBP[ih+1,iw+1] = num\n",
        "  return(imgLBP)\n",
        "\n",
        "# Calculating chi-squared distance\n",
        "#def chiSquared_src(p,q):\n",
        "def chiSquared(p,q):\n",
        "    return 0.5*np.sum((p-q)**2/(p+q+(1e-6)))\n",
        "\n",
        "#vectorized_chiSquared = np.vectorize(chiSquared_src)\n",
        "\n",
        "#def chiSquared(p,q):\n",
        "#  return vectorized_chiSquared(p,q)\n",
        "\n",
        "def flatten_matrices(matrix):\n",
        "  for i in range(0, len(matrix)):\n",
        "      matrix[i] = matrix[i].flatten()\n",
        "  return np.array(matrix)\n",
        "\n",
        "\n",
        "print(\"Objects initialized successfully\")"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Objects initialized successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4mtTesG2RiJ",
        "colab_type": "code",
        "outputId": "a8511181-3dc4-477b-d23e-1466cd0fe39c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "### Main block of initialization ###\n",
        "dataset = []\n",
        "#dataset_images = []\n",
        "indexes = []\n",
        "test_matrix = []\n",
        "train_matrix = []\n",
        "test_labels = []\n",
        "train_labels = []\n",
        "path_x=[]\n",
        "img_list = []\n",
        "train_images = []\n",
        "Table_of_results = []\n",
        "scaler_enum = ['s0_1','s_1_1','s_std']\n",
        "np_axis = 1; # 0 - vertical, 1 - horizontal\n",
        "### Different configurable flag for apply or not some preprocessing ###\n",
        "Basic_use_flag = False           # Basic prediction with MSE\n",
        "### train_test_split function parameters ###\n",
        "random_state = 42                # use as default 42\n",
        "dataset_shuffle = False          # use as default False\n",
        "stratify = None                  # use as default None\n",
        "### Normalization parameters ###\n",
        "PCA_use_flag = False             # do not use PCA if flag is set to False\n",
        "PCA_n_components = 0.99          # Use as default following values: 0.95, 0.98, 0.99\n",
        "Normalization_Funtion = False    # False mean do not use standart normalization, else type any of following functions(string): f_min_max_0_1, f_min_max__1_1, f_mean_std)\n",
        "### model additional parameters ###\n",
        "print_unpredicted_data = False   # print photos that were recognized incorrectly\n",
        "print_distribution = False       # print source photo distribution along the test/train matrices (each step)\n",
        "### LBP parameters ###\n",
        "LBP_use_flag = False\n",
        "### Align parameters ###\n",
        "Align_use_flag = True            # \n",
        "\n",
        "# read folder structure with files and put it to collection\n",
        "folder = []\n",
        "for i in os.walk(os.path.join('ORLFaceRecognition-PCA','att_faces')):\n",
        "  folder.append(i)\n",
        "\n",
        "for address, dirs, files in folder:\n",
        "  for file in fnmatch.filter(files, '*.pgm'):\n",
        "    path_x.append(os.path.join(address,file))\n",
        "\n",
        "for file in sorted(path_x, key=lambda x: int(x.split(\"/\",3)[3].rstrip('.pgm')+x.split(\"/\",3)[2].lstrip(\"s\").zfill(2))):\n",
        "\n",
        "  img = cv2.imread(file)\n",
        "  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)    \n",
        "  gray = gray.astype(float)\n",
        "  person = file.split(\"/\",3)[2].lstrip(\"s\")\n",
        "\n",
        "  #dataset_images.append(img)\n",
        "  \n",
        "  #dataset.append(gray.flatten())\n",
        "  dataset.append(gray)\n",
        "  indexes.append(int(person))\n",
        "  img_list.append(file.split(\"/\",3)[2] + '/' + file.split(\"/\",3)[3].rstrip('.pgm'))\n",
        "\n",
        "img_list = np.array(img_list)\n",
        "\n",
        "#dataset = np.array(dataset)\n",
        "# Normalization block\n",
        "if Normalization_Funtion :\n",
        "  dataset = np.array(dataset)\n",
        "  #dataset=dataset.swapaxes(0,1) # if we want swap axes of input array\n",
        "  \n",
        "  if Normalization_Funtion == 'f_min_max_0_1' :\n",
        "    dataset = np.apply_along_axis(f_min_max_0_1,np_axis,dataset)\n",
        "  elif Normalization_Funtion == 'f_min_max__1_1' :\n",
        "    dataset = np.apply_along_axis(f_min_max__1_1,np_axis,dataset)\n",
        "  elif Normalization_Funtion == 'f_mean_std' :\n",
        "    dataset = np.apply_along_axis(f_mean_std,np_axis,dataset)\n",
        "  # do not apply anything else for basic normalization if names above missing. move to another code block\n",
        "\n",
        "##### Use rotation of images. Eye Align ######\n",
        "if Align_use_flag :\n",
        "\n",
        "  with open('MachineLearning/data_file.json', 'r') as file:\n",
        "      eyes_matrix = json.load(file)\n",
        "\n",
        "  #dataset = np.array(dataset)\n",
        "  #dataset = dataset.astype(float)\n",
        "  dataset = align_images_by_eyes(dataset, eyes_matrix, img_list)\n",
        "  #dataset = np.array(dataset)\n",
        "  dataset = flatten_matrices(dataset)\n",
        "  #dataset = dataset.astype(float)\n",
        "\n",
        "# How to use :\n",
        "# Without shuffle 1,10 (train*test: 90*10, 80*20, etc)\n",
        "# With shuffle 10,19\n",
        "# Both 1,19\n",
        "#for split_mark in range(1, 19):\n",
        "for split_mark in range(1, 10):\n",
        "#for split_mark in range(1, 2): # 90-70% train matrix size, have no sense to check other splits \n",
        "                               # (no valuable information for futher implementation, as well as shuffling)\n",
        "\n",
        "  if split_mark>=10:\n",
        "    dataset_shuffle = True\n",
        "    split_k=split_mark-9\n",
        "  else:\n",
        "    split_k=split_mark\n",
        "\n",
        "  train_matrix, test_matrix, train_labels, test_labels = train_test_split(dataset, indexes, test_size=split_k/10, random_state=random_state, shuffle=dataset_shuffle, stratify=stratify)\n",
        "\n",
        "  # PCA\n",
        "  if PCA_use_flag :\n",
        "    pca = PCA(n_components = PCA_n_components).fit(train_matrix)\n",
        "    train_matrix = pca.transform(train_matrix)\n",
        "    test_matrix = pca.transform(test_matrix)\n",
        "\n",
        "  #print(train_matrix.shape)\n",
        "\n",
        "  #print(train_img)\n",
        "  #print(test_img)\n",
        "  if LBP_use_flag :\n",
        "    Table_of_results = []\n",
        "    #for N in range(1,11) :\n",
        "    for N in range(4,5) :\n",
        "      #for R in range(1,6) :\n",
        "      for R in range(5,6) :\n",
        "        # initialize the local binary patterns descriptor along with\n",
        "        # the data and label lists\n",
        "        print(\"LBP. N: {0}, R: {1}\".format( N, R))\n",
        "        desc = LocalBinaryPatterns(N, R)\n",
        "        data_train = []\n",
        "        labels_train = []\n",
        "        data_test = []\n",
        "        labels_test = []\n",
        "\n",
        "        labels_train = train_labels\n",
        "        labels_test = test_labels\n",
        "\n",
        "        for img in train_matrix :\n",
        "          #print(img.shape)\n",
        "          hist = desc.describe(img)\n",
        "          #hist = desc.getLBPimage(img)\n",
        "          data_train.append(hist)\n",
        "          #print(hist)\n",
        "          #break\n",
        "\n",
        "        for img in test_matrix :\n",
        "          hist = desc.describe(img)\n",
        "          #hist = desc.getLBPimage(img)\n",
        "          data_test.append(hist)\n",
        "\n",
        "        #data_train = data_train.flatten()\n",
        "        #data_test = data_test.flatten()\n",
        "        #print(data_test)\n",
        "        #img_src = data_train[18].reshape(1, -1)\n",
        "        #img_train = data_test[18].reshape(1, -1)\n",
        "        #plt.subplot(1,2,1),plt.imshow(img_src)\n",
        "        #plt.title(\"Test photo\")\n",
        "        #plt.subplot(1,2,2),plt.imshow(img_train)\n",
        "        #plt.title(\"Train photo\")\n",
        "        #plt.show()\n",
        "\n",
        "        model = manyAlgorithmsModel(data_train, data_test, train_labels, test_labels, dataset_shuffle, **{\"originalTrainMatrix\":train_matrix,\"originalTestMatrix\":test_matrix, \"numberOfPoints\": N})\n",
        "\n",
        "        model.predict_chiSquared(print_distribution)\n",
        "        Table_of_results.append([N,R,split_mark*10,split_k*10,model.evaluate_chiSquared()])\n",
        "        model.print_LBP_errors()\n",
        "\n",
        "  if Basic_use_flag :\n",
        "    model = manyAlgorithmsModel(train_matrix, test_matrix, train_labels, test_labels, dataset_shuffle)\n",
        "\n",
        "    model.predict(print_distribution)\n",
        "    model.evaluate()\n",
        "\n",
        "  if Align_use_flag :\n",
        "\n",
        "    model = manyAlgorithmsModel(train_matrix, test_matrix, train_labels, test_labels, dataset_shuffle)\n",
        "\n",
        "    model.predict(print_distribution)\n",
        "    #model.evaluate()\n",
        "    Table_of_results.append([split_mark*10,split_k*10,model.evaluate()])\n",
        "\n",
        "    #model.print_data_errors()\n",
        "\n",
        "# Print LBP results in one.\n",
        "for res in Table_of_results:\n",
        "  print(res)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE.  Train/Test : 90 / 10 - 0.925\n",
            "MAE.  Train/Test : 90 / 10 - 0.950\n",
            "MSE.  Train/Test : 80 / 20 - 0.938\n",
            "MAE.  Train/Test : 80 / 20 - 0.963\n",
            "MSE.  Train/Test : 70 / 30 - 0.942\n",
            "MAE.  Train/Test : 70 / 30 - 0.967\n",
            "MSE.  Train/Test : 60 / 40 - 0.925\n",
            "MAE.  Train/Test : 60 / 40 - 0.950\n",
            "MSE.  Train/Test : 50 / 50 - 0.875\n",
            "MAE.  Train/Test : 50 / 50 - 0.925\n",
            "MSE.  Train/Test : 40 / 60 - 0.850\n",
            "MAE.  Train/Test : 40 / 60 - 0.887\n",
            "MSE.  Train/Test : 30 / 70 - 0.804\n",
            "MAE.  Train/Test : 30 / 70 - 0.868\n",
            "MSE.  Train/Test : 20 / 80 - 0.791\n",
            "MAE.  Train/Test : 20 / 80 - 0.834\n",
            "MSE.  Train/Test : 10 / 90 - 0.697\n",
            "MAE.  Train/Test : 10 / 90 - 0.725\n",
            "[10, 10, (0.925, 0.95)]\n",
            "[20, 20, (0.9375, 0.9625)]\n",
            "[30, 30, (0.9416666666666667, 0.9666666666666667)]\n",
            "[40, 40, (0.925, 0.95)]\n",
            "[50, 50, (0.875, 0.925)]\n",
            "[60, 60, (0.85, 0.8875)]\n",
            "[70, 70, (0.8035714285714286, 0.8678571428571429)]\n",
            "[80, 80, (0.790625, 0.834375)]\n",
            "[90, 90, (0.6972222222222222, 0.725)]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}